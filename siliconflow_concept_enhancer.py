import os
import json
import math
import requests
from typing import List, Dict, Any, Optional, Tuple
from concept_manager import ConceptManager
from ai_processor import AIProcessor

class SiliconFlowConceptEnhancer:
    """åŸºäºSiliconFlow BGE-M3å’Œrerankerçš„æ™ºèƒ½æ¦‚å¿µå¢å¼ºå™¨"""
    
    def __init__(self, api_key: str, ai_processor: AIProcessor, concept_manager: ConceptManager):
        self.api_key = api_key
        self.ai_processor = ai_processor
        self.concept_manager = concept_manager
        
        # APIé…ç½®
        self.embedding_url = "https://api.siliconflow.cn/v1/embeddings"
        self.rerank_url = "https://api.siliconflow.cn/v1/rerank"
        self.embedding_model = "BAAI/bge-m3"
        self.rerank_model = "BAAI/bge-reranker-v2-m3"
        
        # ç¼“å­˜æ–‡ä»¶
        self.embeddings_cache_file = os.path.join(
            concept_manager.vault_path, 
            "æ¦‚å¿µåµŒå…¥ç¼“å­˜_BGE.json"
        )
        self.embeddings_cache = {}
        self.load_embeddings_cache()
    
    def load_embeddings_cache(self) -> None:
        """åŠ è½½åµŒå…¥å‘é‡ç¼“å­˜"""
        try:
            if os.path.exists(self.embeddings_cache_file):
                with open(self.embeddings_cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.embeddings_cache = data.get('embeddings', {})
                print(f"ğŸ“– å·²åŠ è½½ {len(self.embeddings_cache)} ä¸ªæ¦‚å¿µçš„BGEåµŒå…¥å‘é‡")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½åµŒå…¥ç¼“å­˜å¤±è´¥: {e}")
            self.embeddings_cache = {}
    
    def save_embeddings_cache(self) -> None:
        """ä¿å­˜åµŒå…¥å‘é‡ç¼“å­˜"""
        try:
            cache_data = {
                'metadata': {
                    'total_embeddings': len(self.embeddings_cache),
                    'model': self.embedding_model,
                    'last_updated': self._get_current_timestamp()
                },
                'embeddings': self.embeddings_cache
            }
            
            with open(self.embeddings_cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(self.embeddings_cache)} ä¸ªBGEåµŒå…¥å‘é‡")
        except Exception as e:
            print(f"âŒ ä¿å­˜åµŒå…¥ç¼“å­˜å¤±è´¥: {e}")
    
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬çš„BGEåµŒå…¥å‘é‡"""
        # æ£€æŸ¥ç¼“å­˜
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]
        
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "model": self.embedding_model,
                "input": text,
                "encoding_format": "float"
            }
            
            response = requests.post(self.embedding_url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            embedding = result['data'][0]['embedding']
            
            # ç¼“å­˜ç»“æœ
            self.embeddings_cache[text] = embedding
            return embedding
            
        except Exception as e:
            print(f"âŒ è·å–BGEåµŒå…¥å‘é‡å¤±è´¥: {e}")
            return None
    
    def batch_get_embeddings(self, texts: List[str]) -> Dict[str, List[float]]:
        """æ‰¹é‡è·å–åµŒå…¥å‘é‡ï¼ˆæ›´é«˜æ•ˆï¼‰"""
        results = {}
        texts_to_process = []
        
        # æ£€æŸ¥ç¼“å­˜ï¼Œæ”¶é›†éœ€è¦å¤„ç†çš„æ–‡æœ¬
        for text in texts:
            if text in self.embeddings_cache:
                results[text] = self.embeddings_cache[text]
            else:
                texts_to_process.append(text)
        
        if not texts_to_process:
            return results
        
        # æ‰¹é‡å¤„ç†ï¼ˆBGE-M3æ”¯æŒæ‰¹é‡è¾“å…¥ï¼‰
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
            batch_size = 10  # æ¯æ¬¡å¤„ç†10ä¸ª
            
            for i in range(0, len(texts_to_process), batch_size):
                batch_texts = texts_to_process[i:i + batch_size]
                
                data = {
                    "model": self.embedding_model,
                    "input": batch_texts,
                    "encoding_format": "float"
                }
                
                response = requests.post(self.embedding_url, headers=headers, json=data)
                response.raise_for_status()
                
                result = response.json()
                
                # å¤„ç†æ‰¹é‡ç»“æœ
                for j, embedding_data in enumerate(result['data']):
                    text = batch_texts[j]
                    embedding = embedding_data['embedding']
                    
                    # ç¼“å­˜å’Œè¿”å›ç»“æœ
                    self.embeddings_cache[text] = embedding
                    results[text] = embedding
                
                print(f"  ğŸ“Š å·²å¤„ç† {min(i + batch_size, len(texts_to_process))}/{len(texts_to_process)} ä¸ªæ¦‚å¿µ")
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
        
        return results
    
    def rerank_concepts(self, query: str, concept_docs: List[str], top_k: int = 20) -> List[Tuple[int, float]]:
        """ä½¿ç”¨BGE rerankerå¯¹æ¦‚å¿µè¿›è¡Œé‡æ’åº"""
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "model": self.rerank_model,
                "query": query,
                "documents": concept_docs,
                "return_documents": False
            }
            
            response = requests.post(self.rerank_url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            
            # æ£€æŸ¥å“åº”æ ¼å¼ - SiliconFlowä½¿ç”¨'results'è€Œä¸æ˜¯'data'
            if 'results' not in result:
                print(f"    âŒ APIå“åº”ä¸­ç¼ºå°‘'results'å­—æ®µ: {list(result.keys())}")
                return []
            
            # æå–æ’åºç»“æœ
            ranked_results = []
            for item in result['results']:
                if 'index' not in item or 'relevance_score' not in item:
                    print(f"    âš ï¸ æ’åºé¡¹ç¼ºå°‘å¿…è¦å­—æ®µ: {item}")
                    continue
                    
                index = item['index']
                score = item['relevance_score']  # SiliconFlowä½¿ç”¨relevance_score
                
                # SiliconFlowçš„åˆ†æ•°å·²ç»æ˜¯0-1èŒƒå›´ï¼Œä¸éœ€è¦sigmoidè½¬æ¢
                ranked_results.append((index, score))
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
            ranked_results.sort(key=lambda x: x[1], reverse=True)
            
            print(f"    ğŸ“Š é‡æ’åºå¾—åˆ†åˆ†å¸ƒ: æœ€é«˜{ranked_results[0][1]:.3f}, æœ€ä½{ranked_results[-1][1]:.3f}")
            
            return ranked_results[:top_k]
            
        except requests.exceptions.HTTPError as e:
            print(f"âŒ BGEé‡æ’åºHTTPé”™è¯¯: {e}")
            print(f"    å“åº”çŠ¶æ€ç : {response.status_code}")
            try:
                error_detail = response.json()
                print(f"    é”™è¯¯è¯¦æƒ…: {error_detail}")
            except:
                print(f"    å“åº”å†…å®¹: {response.text}")
            return []
        except requests.exceptions.RequestException as e:
            print(f"âŒ BGEé‡æ’åºè¯·æ±‚å¤±è´¥: {e}")
            return []
        except json.JSONDecodeError as e:
            print(f"âŒ BGEé‡æ’åºå“åº”è§£æå¤±è´¥: {e}")
            print(f"    å“åº”å†…å®¹: {response.text}")
            return []
        except Exception as e:
            print(f"âŒ BGEé‡æ’åºæœªçŸ¥é”™è¯¯: {e}")
            return []
    
    def build_concept_embeddings(self, force_rebuild: bool = False) -> None:
        """ä¸ºæ‰€æœ‰æ¦‚å¿µæ„å»ºBGEåµŒå…¥å‘é‡"""
        print("ğŸ”„ æ„å»ºæ¦‚å¿µBGEåµŒå…¥å‘é‡...")
        
        if not self.concept_manager.concept_database:
            print("âŒ æ¦‚å¿µæ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆæ‰«æç¬”è®°")
            return
        
        concepts_to_process = []
        
        for concept_name, concept_data in self.concept_manager.concept_database.items():
            if force_rebuild or concept_name not in self.embeddings_cache:
                concepts_to_process.append((concept_name, concept_data))
        
        if not concepts_to_process:
            print("âœ… æ‰€æœ‰æ¦‚å¿µçš„BGEåµŒå…¥å‘é‡å·²å­˜åœ¨")
            return
        
        print(f"ğŸ“ éœ€è¦å¤„ç† {len(concepts_to_process)} ä¸ªæ¦‚å¿µ")
        
        # å‡†å¤‡æ‰¹é‡æ–‡æœ¬
        concept_texts = []
        concept_names = []
        
        for concept_name, concept_data in concepts_to_process:
            concept_text = self._build_concept_description(concept_name, concept_data)
            concept_texts.append(concept_text)
            concept_names.append(concept_name)
        
        # æ‰¹é‡è·å–åµŒå…¥å‘é‡
        print("ğŸš€ æ‰¹é‡è·å–BGEåµŒå…¥å‘é‡...")
        embeddings = self.batch_get_embeddings(concept_texts)
        
        # æ˜ å°„å›æ¦‚å¿µåç§°
        for i, concept_name in enumerate(concept_names):
            concept_text = concept_texts[i]
            if concept_text in embeddings:
                self.embeddings_cache[concept_name] = embeddings[concept_text]
        
        # ä¿å­˜ç¼“å­˜
        self.save_embeddings_cache()
        print("âœ… æ¦‚å¿µBGEåµŒå…¥å‘é‡æ„å»ºå®Œæˆ")
    
    def _build_concept_description(self, concept_name: str, concept_data: Dict) -> str:
        """æ„å»ºæ¦‚å¿µçš„æè¿°æ–‡æœ¬ç”¨äºåµŒå…¥"""
        parts = [concept_name]
        
        # æ·»åŠ åˆ«å
        if concept_data.get('aliases'):
            parts.extend(concept_data['aliases'])
        
        # æ·»åŠ ç§‘ç›®ä¿¡æ¯
        if concept_data.get('subject'):
            parts.append(f"æ³•è€ƒ{concept_data['subject']}")
        
        # æ·»åŠ æ ‡ç­¾
        if concept_data.get('tags'):
            parts.extend([tag for tag in concept_data['tags'] if tag])
        
        # æ·»åŠ ç›¸å…³æ¦‚å¿µï¼ˆæœ‰é™æ•°é‡ï¼‰
        if concept_data.get('related_concepts'):
            parts.extend(concept_data['related_concepts'][:3])  # åªå–å‰3ä¸ª
        
        return ' '.join(parts)
    
    def find_related_concepts_hybrid(
        self, 
        note_content: str, 
        note_title: str,
        embedding_top_k: int = 100,
        rerank_top_k: int = 15,
        rerank_threshold: float = 0.98  # è°ƒæ•´é»˜è®¤é˜ˆå€¼é€‚åº”BGEç‰¹æ€§
    ) -> List[Tuple[str, float]]:
        """
        æ··åˆæ£€ç´¢ï¼šå…ˆç”¨BGE embeddingå¬å›ï¼Œå†ç”¨rerankerç²¾æ’
        
        Args:
            note_content: ç¬”è®°å†…å®¹
            note_title: ç¬”è®°æ ‡é¢˜
            embedding_top_k: embeddingå¬å›çš„æ•°é‡
            rerank_top_k: rerankerç²¾æ’åè¿”å›çš„æ•°é‡
            rerank_threshold: rerankeråˆ†æ•°é˜ˆå€¼
            
        Returns:
            (æ¦‚å¿µå, rerankeråˆ†æ•°) çš„åˆ—è¡¨
        """
        # 1. æ„å»ºæŸ¥è¯¢æ–‡æœ¬
        query_text = f"{note_title} {note_content}"
        
        # 2. BGE embeddingå¬å›é˜¶æ®µ
        print(f"ğŸ” BGE embeddingå¬å› top-{embedding_top_k}...")
        
        query_embedding = self.get_embedding(query_text)
        if query_embedding is None:
            return []
        
        # è®¡ç®—ä¸æ‰€æœ‰æ¦‚å¿µçš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for concept_name in self.concept_manager.concept_database.keys():
            if concept_name == note_title:  # è·³è¿‡è‡ªå·±
                continue
                
            if concept_name not in self.embeddings_cache:
                continue
            
            concept_embedding = self.embeddings_cache[concept_name]
            similarity = self._cosine_similarity(query_embedding, concept_embedding)
            similarities.append((concept_name, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶å–top_k
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_concepts = similarities[:embedding_top_k]
        
        print(f"  ğŸ“Š å¬å› {len(top_concepts)} ä¸ªå€™é€‰æ¦‚å¿µ")
        
        if not top_concepts:
            return []
        
        # 3. BGE rerankerç²¾æ’é˜¶æ®µ
        print(f"ğŸ¯ BGE rerankerç²¾æ’ top-{rerank_top_k}...")
        
        # å‡†å¤‡rerankerè¾“å…¥
        concept_names = [concept for concept, _ in top_concepts]
        concept_docs = []
        
        for concept_name in concept_names:
            concept_data = self.concept_manager.concept_database.get(concept_name, {})
            concept_doc = self._build_concept_description(concept_name, concept_data)
            concept_docs.append(concept_doc)
        
        # è°ƒç”¨reranker
        rerank_results = self.rerank_concepts(query_text, concept_docs, rerank_top_k)
        
        # æ˜ å°„å›æ¦‚å¿µåç§°å¹¶è¿‡æ»¤
        final_results = []
        for doc_index, rerank_score in rerank_results:
            if rerank_score >= rerank_threshold:
                concept_name = concept_names[doc_index]
                final_results.append((concept_name, rerank_score))
        
        # å¦‚æœé˜ˆå€¼è¿‡é«˜å¯¼è‡´ç»“æœå¤ªå°‘ï¼ŒåŠ¨æ€è°ƒæ•´
        if len(final_results) < 3 and rerank_results:
            # å–å‰5ä¸ªæœ€é«˜åˆ†çš„ï¼Œä¸ç®¡é˜ˆå€¼
            print(f"    ğŸ”„ é˜ˆå€¼{rerank_threshold}è¿‡é«˜ï¼ŒåŠ¨æ€è°ƒæ•´ä¸ºå‰5ä¸ªæœ€é«˜åˆ†æ¦‚å¿µ")
            final_results = []
            for doc_index, rerank_score in rerank_results[:5]:
                concept_name = concept_names[doc_index]
                final_results.append((concept_name, rerank_score))
        
        print(f"  âœ¨ ç²¾æ’åä¿ç•™ {len(final_results)} ä¸ªé«˜è´¨é‡æ¦‚å¿µ")
        
        # æ˜¾ç¤ºtop 5ç»“æœ
        for i, (concept, score) in enumerate(final_results[:5]):
            print(f"    {i+1}. {concept}: {score:.4f}")  # æé«˜ç²¾åº¦æ˜¾ç¤º
        
        return final_results
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            import numpy as np
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except Exception:
            return 0.0
    
    def enhance_note_with_hybrid_search(
        self, 
        note_content: str, 
        note_title: str,
        embedding_top_k: int = 100,
        rerank_top_k: int = 15,
        rerank_threshold: float = 0.98  # è°ƒæ•´é»˜è®¤é˜ˆå€¼
    ) -> Optional[Dict]:
        """
        ä½¿ç”¨æ··åˆæ£€ç´¢å¢å¼ºå•ä¸ªç¬”è®°çš„æ¦‚å¿µå…³ç³»
        """
        print(f"ğŸ” æ··åˆæ£€ç´¢æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µ...")
        
        # 1. æ··åˆæ£€ç´¢æ‰¾åˆ°æœ€ç›¸å…³çš„æ¦‚å¿µ
        related_concepts = self.find_related_concepts_hybrid(
            note_content, note_title, embedding_top_k, rerank_top_k, rerank_threshold
        )
        
        if not related_concepts:
            print(f"  âš ï¸ æœªæ‰¾åˆ°é‡æ’åºåˆ†æ•°é«˜äº {rerank_threshold} çš„ç›¸å…³æ¦‚å¿µ")
            return {'modified': False}
        
        # 2. æ„å»ºç²¾ç®€çš„æ¦‚å¿µåˆ—è¡¨ç»™AI
        filtered_concepts = {
            'existing_concepts': [concept for concept, _ in related_concepts],
            'concept_aliases': {},
            'concept_relationships': {},
            'rerank_scores': {concept: score for concept, score in related_concepts}
        }
        
        # æ·»åŠ æ¦‚å¿µçš„è¯¦ç»†ä¿¡æ¯
        for concept_name, _ in related_concepts:
            if concept_name in self.concept_manager.concept_database:
                data = self.concept_manager.concept_database[concept_name]
                filtered_concepts['concept_aliases'][concept_name] = data.get('aliases', [])
                filtered_concepts['concept_relationships'][concept_name] = data.get('related_concepts', [])
        
        # 3. è°ƒç”¨AIè¿›è¡Œç²¾ç¡®åˆ†æ
        print(f"ğŸ¤– AIåˆ†ææœ€ç›¸å…³çš„ {len(related_concepts)} ä¸ªæ¦‚å¿µ...")
        
        enhanced_prompt = self._build_hybrid_enhanced_prompt(
            note_content, note_title, filtered_concepts
        )
        
        try:
            response = self.ai_processor.client.chat.completions.create(
                model=self.ai_processor.model,
                messages=[{"role": "user", "content": enhanced_prompt}],
                temperature=0,
            )
            
            result = self.ai_processor._parse_single_note_enhancement_response(
                response.choices[0].message.content, 
                note_content
            )
            
            if result and result.get('modified'):
                print(f"  âœ… AIå»ºè®®ä¿®æ”¹")
            else:
                print(f"  âš ï¸ AIè®¤ä¸ºæ— éœ€ä¿®æ”¹")
            
            return result
            
        except Exception as e:
            print(f"âŒ AIå¢å¼ºå¤±è´¥: {e}")
            return {'modified': False}
    
    def _build_hybrid_enhanced_prompt(
        self, 
        note_content: str, 
        note_title: str, 
        filtered_concepts: Dict
    ) -> str:
        """æ„å»ºåŸºäºæ··åˆæ£€ç´¢çš„å¢å¼ºæç¤ºè¯"""
        
        concepts_info = []
        for concept in filtered_concepts['existing_concepts']:
            aliases = filtered_concepts['concept_aliases'].get(concept, [])
            score = filtered_concepts['rerank_scores'].get(concept, 0.0)
            
            info = f"- {concept} (ç›¸å…³åº¦: {score:.3f})"
            if aliases:
                info += f" [åˆ«å: {', '.join(aliases[:2])}]"
            concepts_info.append(info)
        
        concepts_list = '\n'.join(concepts_info)
        
        return f"""
(defun æ³•è€ƒæ¦‚å¿µé“¾æ¥ä¸“å®¶ ()
  "åŸºäºæ¦‚å¿µåº“ä¼˜åŒ–æ³•è€ƒç¬”è®°æ¦‚å¿µå…³ç³»"
  (èº«ä»½ . æ³•è€ƒçŸ¥è¯†ä½“ç³»ä¸“å®¶)
  (ä¸“é•¿ . (æ¦‚å¿µå…³ç³»åˆ†æ çŸ¥è¯†å›¾è°±æ„å»º è¯­ä¹‰ç›¸å…³æ€§åˆ¤æ–­))
  (å·¥å…·ç®± . (æ¦‚å¿µåº“åŒ¹é… åŒé“¾è¯­æ³•è§„èŒƒ å…³è”æ€§åˆ†æ))
  (è´¨é‡æ ‡å‡† . (é«˜åº¦ç›¸å…³æ€§ ä¸¥æ ¼æŠŠæ§ æ¦‚å¿µåº“å‡†ç¡®æ€§))
  (é“¾æ¥æ ¼å¼ . "[[ã€ç§‘ç›®ã€‘æ¦‚å¿µå|æ¦‚å¿µå/åˆ«å]]"))

(defun æ¦‚å¿µå…³ç³»åˆ†æå™¨ (ç¬”è®°æ ‡é¢˜ ç¬”è®°å†…å®¹ æ¦‚å¿µåº“)
  "åˆ†æç¬”è®°ä¸æ¦‚å¿µåº“çš„å…³è”æ€§ï¼Œä¼˜åŒ–æ¦‚å¿µé“¾æ¥"
  (let* ((å†…å®¹è§£æ (è¯­ä¹‰åˆ†æ ç¬”è®°å†…å®¹))
         (ç°æœ‰é“¾æ¥æ£€æŸ¥ (éªŒè¯ç°æœ‰é“¾æ¥ ç¬”è®°å†…å®¹ æ¦‚å¿µåº“))
         (é—æ¼æ¦‚å¿µè¯†åˆ« (å‘ç°ç›¸å…³æ¦‚å¿µ å†…å®¹è§£æ æ¦‚å¿µåº“))
         (å…³ç³»åˆ†ç±» (æ¦‚å¿µåˆ†ç±»å™¨ é—æ¼æ¦‚å¿µè¯†åˆ«))
         (é“¾æ¥ä¼˜åŒ– (åŒé“¾æ ¼å¼åŒ– å…³ç³»åˆ†ç±» ç°æœ‰é“¾æ¥æ£€æŸ¥)))
    (é™é»˜è¾“å‡º é“¾æ¥ä¼˜åŒ–)))

(defun éªŒè¯ç°æœ‰é“¾æ¥ (ç¬”è®°å†…å®¹ æ¦‚å¿µåº“)
  "æ£€æŸ¥ç¬”è®°ä¸­ç°æœ‰çš„[[æ¦‚å¿µ]]é“¾æ¥æ˜¯å¦å‡†ç¡®"
  (let ((ç°æœ‰é“¾æ¥ (æå–ç°æœ‰é“¾æ¥ ç¬”è®°å†…å®¹)))
    (filter (lambda (é“¾æ¥)
              (æ¦‚å¿µåº“ä¸­å­˜åœ¨? é“¾æ¥ æ¦‚å¿µåº“))
            ç°æœ‰é“¾æ¥)))

(defun å‘ç°ç›¸å…³æ¦‚å¿µ (ç¬”è®°è¯­ä¹‰ æ¦‚å¿µåº“)
  "è¯†åˆ«ç¬”è®°ä¸­å¯èƒ½é—æ¼çš„é‡è¦æ¦‚å¿µå…³è”"
  (filter (lambda (æ¦‚å¿µ)
            (and (æ¦‚å¿µåº“ä¸­å­˜åœ¨? æ¦‚å¿µ æ¦‚å¿µåº“)
                 (or (ç›´æ¥æåŠ? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰)
                     (è¯­ä¹‰ç›¸å…³? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰)
                     (ä¸Šä¸‹çº§å…³ç³»? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰)
                     (å¹¶åˆ—å…³ç³»? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰))))
          æ¦‚å¿µåº“))

(defun æ¦‚å¿µåˆ†ç±»å™¨ (ç›¸å…³æ¦‚å¿µ)
  "æŒ‰ç…§å…³ç³»ç±»å‹å¯¹æ¦‚å¿µè¿›è¡Œåˆ†ç±»"
  (let ((åˆ†ç±»ç»“æœ '()))
    (dolist (æ¦‚å¿µ ç›¸å…³æ¦‚å¿µ)
      (cond ((æ ¸å¿ƒç›¸å…³? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :æ ¸å¿ƒ)))
            ((æ„æˆè¦ä»¶ç›¸å…³? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :è¦ä»¶)))
            ((æ‰¿æ‹…æ–¹å¼ç›¸å…³? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :æ–¹å¼)))
            ((å¯¹æ¯”æ¦‚å¿µ? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :å¯¹æ¯”)))))
    åˆ†ç±»ç»“æœ))

(defun é™é»˜è¾“å‡º (ä¼˜åŒ–ç»“æœ)
  "é™é»˜æ‰§è¡Œï¼Œä»…è¾“å‡ºæ ‡å‡†æ ¼å¼ç»“æœï¼Œç¦æ­¢ä»»ä½•ä»£ç å—åŒ…è£¹"
  (cond ((éœ€è¦ä¿®æ”¹? ä¼˜åŒ–ç»“æœ)
         (format nil "MODIFIED: true~%ENHANCED_CONTENT:~%~a" 
                 (ç›´æ¥è¾“å‡ºæ–‡å­—å†…å®¹ ä¼˜åŒ–ç»“æœ)))
        (t "MODIFIED: false")))

(defun ç›´æ¥è¾“å‡ºæ–‡å­—å†…å®¹ (ä¼˜åŒ–ç»“æœ)
  "ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬å†…å®¹ï¼Œä¸ä½¿ç”¨ä»»ä½•markdownä»£ç å—åŒ…è£¹"
  (ç”Ÿæˆå®Œæ•´å†…å®¹ ä¼˜åŒ–ç»“æœ))

(defun é™é»˜æ‰§è¡Œæ¨¡å¼ ()
  "è®¾ç½®é™é»˜æ‰§è¡Œçº¦æŸ"
  '((ç¦æ­¢è§£é‡Š . t)
    (ç¦æ­¢åˆ†æè¿‡ç¨‹è¾“å‡º . t) 
    (ç¦æ­¢é¢å¤–è¯´æ˜ . t)
    (ä»…è¾“å‡ºç»“æœ . t)
    (ä¸¥æ ¼æ ¼å¼æ§åˆ¶ . t)
    (ç¦æ­¢ä»£ç å—åŒ…è£¹ . t)))

(defun è´¨é‡æ§åˆ¶è§„åˆ™ ()
  "ç¡®ä¿è¾“å‡ºè´¨é‡çš„è§„åˆ™é›†"
  '((æ¦‚å¿µåº“å­˜åœ¨æ€§ . "åªæ·»åŠ ç¡®å®å­˜åœ¨äºæ¦‚å¿µåº“ä¸­çš„æ¦‚å¿µé“¾æ¥")
    (é«˜åº¦ç›¸å…³æ€§ . "ç¡®ä¿æ–°å¢çš„æ¦‚å¿µé“¾æ¥ä¸ç¬”è®°å†…å®¹é«˜åº¦ç›¸å…³")
    (ç°æœ‰é“¾æ¥éªŒè¯ . "æ£€æŸ¥ç¬”è®°ä¸­ç°æœ‰çš„[[æ¦‚å¿µ]]é“¾æ¥æ˜¯å¦å‡†ç¡®")
    (æ ¼å¼æ ‡å‡† . "ä¸¥æ ¼ä½¿ç”¨åŒé“¾æ˜¾ç¤ºåˆ«åæ ¼å¼")
    (ç§»é™¤æ— å…³ . "ç§»é™¤æŒ‡å‘ä¸å­˜åœ¨æ¦‚å¿µçš„é“¾æ¥")))

(defun start ()
  "å¯åŠ¨é™é»˜æ‰§è¡Œæ¨¡å¼"
  (setq system-role æ³•è€ƒæ¦‚å¿µé“¾æ¥ä¸“å®¶)
  (setq execution-mode (é™é»˜æ‰§è¡Œæ¨¡å¼))
  (setq quality-rules (è´¨é‡æ§åˆ¶è§„åˆ™))
  (setq output-only t))

;; æ‰§è¡Œçº¦æŸ
;; 1. å¿…é¡»å¯åŠ¨é™é»˜æ‰§è¡Œæ¨¡å¼
;; 2. ç¦æ­¢è¾“å‡ºä»»ä½•åˆ†æè¿‡ç¨‹ã€è§£é‡Šæˆ–è¯´æ˜
;; 3. ç¦æ­¢è¾“å‡ºæ€è€ƒè¿‡ç¨‹æˆ–ä¸­é—´ç»“æœ
;; 4. ä¸¥æ ¼è¾“å‡ºæ ¼å¼ï¼šä»… "MODIFIED: true/false" å’Œå¯¹åº”å†…å®¹
;; 5. ä»»ä½•éæ ‡å‡†æ ¼å¼çš„è¾“å‡ºéƒ½æ˜¯é”™è¯¯çš„
;; 6. ä¸å¾—æ·»åŠ å¼•è¨€ã€æ€»ç»“ã€è§£é‡Šæˆ–å…¶ä»–æ–‡å­—
;; 7. ç¦æ­¢ä½¿ç”¨```ä»£ç å—åŒ…è£¹è¾“å‡ºå†…å®¹

;; ========== Pythonè„šæœ¬æ¨¡æ¿æ¥å£ ==========

è¯·åˆ†æä»¥ä¸‹ç¬”è®°ï¼Œæ£€æŸ¥å¹¶ä¼˜åŒ–å…¶æ¦‚å¿µå…³ç³»é“¾æ¥ï¼š

**ç¬”è®°æ ‡é¢˜ï¼š**{note_title}

**ç¬”è®°å†…å®¹ï¼š**
{note_content}

**ç°æœ‰æ¦‚å¿µåº“ï¼š**
{concepts_list}

æ‰§è¡Œåˆ†æä»»åŠ¡ï¼š(æ¦‚å¿µå…³ç³»åˆ†æå™¨ "{note_title}" ç¬”è®°å†…å®¹ æ¦‚å¿µåº“)"""
    
    def batch_enhance_with_hybrid_search(
        self, 
        notes: List[Dict[str, Any]],
        rebuild_embeddings: bool = False,
        embedding_top_k: int = 100,
        rerank_top_k: int = 15,
        rerank_threshold: float = 0.98  # è°ƒæ•´é»˜è®¤é˜ˆå€¼
    ) -> Dict[str, int]:
        """
        ä½¿ç”¨æ··åˆæ£€ç´¢æ‰¹é‡å¢å¼ºç¬”è®°
        """
        print("ğŸš€ å¯åŠ¨åŸºäºBGEæ··åˆæ£€ç´¢çš„ç¬”è®°å¢å¼º...")
        
        # 1. ç¡®ä¿åµŒå…¥å‘é‡å·²æ„å»º
        self.build_concept_embeddings(force_rebuild=rebuild_embeddings)
        
        # 2. æ‰¹é‡å¤„ç†ç¬”è®°
        stats = {
            'total': len(notes),
            'enhanced': 0,
            'unchanged': 0,
            'failed': 0
        }
        
        for i, note_info in enumerate(notes, 1):
            print(f"\nğŸ”„ å¤„ç† {i}/{len(notes)}: {note_info['title']}")
            
            try:
                result = self.enhance_note_with_hybrid_search(
                    note_info['content'], 
                    note_info['title'],
                    embedding_top_k,
                    rerank_top_k,
                    rerank_threshold
                )
                
                if result and result.get('modified'):
                    # åº”ç”¨ä¿®æ”¹
                    if self._apply_enhancement(note_info, result):
                        stats['enhanced'] += 1
                        print(f"  âœ… å¢å¼ºæˆåŠŸ")
                    else:
                        stats['failed'] += 1
                        print(f"  âŒ åº”ç”¨ä¿®æ”¹å¤±è´¥")
                else:
                    stats['unchanged'] += 1
                    print(f"  âš ï¸ æ— éœ€ä¿®æ”¹")
                    
            except Exception as e:
                stats['failed'] += 1
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        print(f"\nğŸ‰ åŸºäºBGEæ··åˆæ£€ç´¢çš„æ‰¹é‡å¢å¼ºå®Œæˆï¼")
        print(f"  ğŸ“Š æ€»è®¡: {stats['total']} ä¸ªç¬”è®°")
        print(f"  âœ… æˆåŠŸå¢å¼º: {stats['enhanced']} ä¸ª")
        print(f"  âš ï¸ æ— éœ€ä¿®æ”¹: {stats['unchanged']} ä¸ª")
        print(f"  âŒ å¤„ç†å¤±è´¥: {stats['failed']} ä¸ª")
        
        return stats
    
    def _apply_enhancement(self, note_info: Dict[str, Any], result: Dict) -> bool:
        """åº”ç”¨å¢å¼ºç»“æœåˆ°æ–‡ä»¶"""
        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_path = note_info['file_path'] + '.backup'
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(note_info['content'])
            
            # å†™å…¥å¢å¼ºåçš„å†…å®¹
            with open(note_info['file_path'], 'w', encoding='utf-8') as f:
                f.write(result['enhanced_content'])
            
            # åˆ é™¤å¤‡ä»½æ–‡ä»¶
            os.remove(backup_path)
            return True
            
        except Exception as e:
            print(f"âŒ åº”ç”¨å¢å¼ºå¤±è´¥: {e}")
            return False
    
    def _get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


# config.py ä¸­éœ€è¦æ·»åŠ SiliconFlowé…ç½®
class Config:
    # ç°æœ‰é…ç½®...
    
    # SiliconFlow APIé…ç½®
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY", "your_siliconflow_api_key")


# ä¸»ç¨‹åºé›†æˆä»£ç ç‰‡æ®µ
def integrate_siliconflow_enhancer():
    """é›†æˆSiliconFlowå¢å¼ºå™¨åˆ°ä¸»ç¨‹åºçš„ç¤ºä¾‹ä»£ç """
    
    # åœ¨ main.py çš„ LawExamNoteProcessor ç±»ä¸­ä¿®æ”¹ï¼š
    
    def __init__(self):
        # ... ç°æœ‰åˆå§‹åŒ–ä»£ç  ...
        
        # æ·»åŠ SiliconFlowå¢å¼ºå™¨
        self.siliconflow_enhancer = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    def _get_siliconflow_enhancer(self):
        """è·å–SiliconFlowå¢å¼ºå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self.siliconflow_enhancer is None:
            from siliconflow_concept_enhancer import SiliconFlowConceptEnhancer
            self.siliconflow_enhancer = SiliconFlowConceptEnhancer(
                Config.SILICONFLOW_API_KEY,
                self.ai_processor, 
                self.concept_manager
            )
        return self.siliconflow_enhancer
    
    def _enhance_with_hybrid_search(self):
        """ä½¿ç”¨BGEæ··åˆæ£€ç´¢å¢å¼ºç¬”è®°"""
        print("\nğŸ”– BGEæ··åˆæ£€ç´¢æ¨¡å¼ï¼ˆembeddingå¬å›+rerankerç²¾æ’ï¼‰")
        
        enhancer = self._get_siliconflow_enhancer()
        
        print("å‚æ•°é…ç½®:")
        print("1. ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆå¬å›100ä¸ªï¼Œç²¾æ’15ä¸ªï¼Œé˜ˆå€¼0.3ï¼‰")
        print("2. è‡ªå®šä¹‰å‚æ•°")
        print("3. è¿”å›")
        
        config_choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        if config_choice == '2':
            try:
                embedding_top_k = int(input("embeddingå¬å›æ•°é‡ (å»ºè®®50-200): ") or "100")
                rerank_top_k = int(input("rerankerç²¾æ’æ•°é‡ (å»ºè®®10-20): ") or "15")
                rerank_threshold = float(input("rerankeråˆ†æ•°é˜ˆå€¼ (å»ºè®®0.2-0.5): ") or "0.3")
            except ValueError:
                print("âŒ å‚æ•°æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                embedding_top_k, rerank_top_k, rerank_threshold = 100, 15, 0.3
        elif config_choice == '1':
            embedding_top_k, rerank_top_k, rerank_threshold = 100, 15, 0.3
        else:
            return
        
        print("1. å¢å¼ºæ‰€æœ‰ç§‘ç›®çš„ç¬”è®°")
        print("2. å¢å¼ºç‰¹å®šç§‘ç›®çš„ç¬”è®°")
        print("3. è¿”å›")
        
        choice = input("è¯·é€‰æ‹©æ“ä½œ (1-3): ").strip()
        
        if choice == '1':
            notes = self._collect_all_law_notes()
            if notes:
                enhancer.batch_enhance_with_hybrid_search(
                    notes, False, embedding_top_k, rerank_top_k, rerank_threshold
                )
        elif choice == '2':
            subject = self._select_subject()
            if subject:
                notes = self._collect_subject_notes_by_name(subject)
                if notes:
                    enhancer.batch_enhance_with_hybrid_search(
                        notes, False, embedding_top_k, rerank_top_k, rerank_threshold
                    )
        
        # é‡æ–°æ‰«ææ›´æ–°æ¦‚å¿µæ•°æ®åº“
        if choice in ['1', '2']:
            print(f"\nğŸ“š é‡æ–°æ‰«ææ›´æ–°æ¦‚å¿µæ•°æ®åº“...")
            self.concept_manager.scan_existing_notes()