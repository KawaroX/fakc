import os
import json
import math
import requests
import hashlib
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from concept_manager import ConceptManager
from ai_processor import AIProcessor
from incremental_processor import IncrementalProcessor
from reverse_linker import ReverseLinker

class SiliconFlowConceptEnhancer:
    """åŸºäºSiliconFlow BGE-M3å’Œrerankerçš„æ™ºèƒ½æ¦‚å¿µå¢å¼ºå™¨"""
    
    def __init__(self, api_key: str, ai_processor: AIProcessor, concept_manager: ConceptManager):
        self.api_key = api_key
        self.ai_processor = ai_processor
        self.concept_manager = concept_manager
        
        # APIé…ç½®
        self.embedding_url = "https://api.siliconflow.cn/v1/embeddings"
        self.rerank_url = "https://api.siliconflow.cn/v1/rerank"
        self.embedding_model = "BAAI/bge-m3"
        self.rerank_model = "BAAI/bge-reranker-v2-m3"
        
        # # ç¼“å­˜æ–‡ä»¶
        # self.embeddings_cache_file = os.path.join(
        #     concept_manager.vault_path, 
        #     "æ¦‚å¿µåµŒå…¥ç¼“å­˜_BGE.json"
        # )
        # self.embeddings_cache = {}
        # self.load_embeddings_cache()

        # ç¼“å­˜æ–‡ä»¶ - v2
        self.embeddings_cache_file = os.path.join(
            concept_manager.vault_path, 
            "enhanced_bge_cache_v2.json"
        )
        self.cache_data = {
            'metadata': {
                'version': '2.0',
                'model': 'BAAI/bge-m3',
                'created': None,
                'last_updated': None,
                'total_concepts': 0
            },
            'concept_hashes': {},  # æ¦‚å¿µå -> å†…å®¹å“ˆå¸Œ
            'embeddings': {},      # å†…å®¹å“ˆå¸Œ -> åµŒå…¥å‘é‡
            'concept_metadata': {} # æ¦‚å¿µå -> å…ƒæ•°æ®
        }
        self.load_embeddings_cache()

        self.incremental_processor = IncrementalProcessor(concept_manager.vault_path)
        self.reverse_linker = ReverseLinker(self, self.concept_manager)
    
    # def load_embeddings_cache(self) -> None:
    #     """åŠ è½½åµŒå…¥å‘é‡ç¼“å­˜"""
    #     try:
    #         if os.path.exists(self.embeddings_cache_file):
    #             with open(self.embeddings_cache_file, 'r', encoding='utf-8') as f:
    #                 data = json.load(f)
    #                 self.embeddings_cache = data.get('embeddings', {})
    #             print(f"ğŸ“– å·²åŠ è½½ {len(self.embeddings_cache)} ä¸ªæ¦‚å¿µçš„BGEåµŒå…¥å‘é‡")
    #     except Exception as e:
    #         print(f"âš ï¸ åŠ è½½åµŒå…¥ç¼“å­˜å¤±è´¥: {e}")
    #         self.embeddings_cache = {}

    def load_embeddings_cache(self) -> None:
        """åŠ è½½å¢å¼ºå‹åµŒå…¥å‘é‡ç¼“å­˜ - v2"""
        try:
            if os.path.exists(self.embeddings_cache_file):
                with open(self.embeddings_cache_file, 'r', encoding='utf-8') as f:
                    loaded_data = json.load(f)
                    
                # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
                if loaded_data.get('metadata', {}).get('version') == '2.0':
                    self.cache_data = loaded_data
                    print(f"ğŸ“– å·²åŠ è½½å¢å¼ºå‹ç¼“å­˜ v2.0: {len(self.cache_data['embeddings'])} ä¸ªåµŒå…¥å‘é‡")
                else:
                    # æ—§ç‰ˆæœ¬ç¼“å­˜ï¼Œéœ€è¦è¿ç§»
                    print("ğŸ”„ æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬ç¼“å­˜ï¼Œæ­£åœ¨è¿ç§»...")
                    self._migrate_old_cache(loaded_data)
            else:
                # åˆå§‹åŒ–æ–°ç¼“å­˜
                self.cache_data['metadata']['created'] = datetime.now().isoformat()
                print("âœ¨ åˆå§‹åŒ–æ–°çš„å¢å¼ºå‹ç¼“å­˜ v2.0")
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            # é‡ç½®ä¸ºé»˜è®¤ç»“æ„
            self.cache_data['metadata']['created'] = datetime.now().isoformat()

    def _migrate_old_cache(self, old_data: dict) -> None:
        """è¿ç§»æ—§ç‰ˆæœ¬ç¼“å­˜åˆ°æ–°æ ¼å¼"""
        try:
            if 'embeddings' in old_data:
                migrated_count = 0
                for text, embedding in old_data['embeddings'].items():
                    # ä¸ºæ—§ç¼“å­˜é¡¹ç”Ÿæˆç¨³å®šçš„å“ˆå¸Œé”®
                    cache_key = self._get_stable_cache_key("unknown", text)
                    self.cache_data['embeddings'][cache_key] = embedding
                    migrated_count += 1
                
                print(f"âœ… æˆåŠŸè¿ç§» {migrated_count} ä¸ªç¼“å­˜é¡¹")
                self.cache_data['metadata']['total_concepts'] = migrated_count
                self.save_embeddings_cache()
        except Exception as e:
            print(f"âŒ ç¼“å­˜è¿ç§»å¤±è´¥: {e}")

    def _get_stable_cache_key(self, concept_name: str, concept_content: str) -> str:
        """ç”Ÿæˆç¨³å®šçš„ç¼“å­˜é”®"""
        
        # æ ‡å‡†åŒ–æ¦‚å¿µå†…å®¹
        normalized = self._normalize_content(concept_content)
        # ä½¿ç”¨æ¦‚å¿µå+å†…å®¹å“ˆå¸Œä½œä¸ºç¨³å®šé”®
        content_hash = hashlib.md5(normalized.encode('utf-8')).hexdigest()
        return f"{concept_name}#{content_hash}"

    def _normalize_content(self, content: str) -> str:
        """æ ‡å‡†åŒ–å†…å®¹ï¼Œç§»é™¤ä¼šå½±å“ç¼“å­˜çš„å¾®å°å˜åŒ–"""
        import re
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç¬¦æ ‡å‡†åŒ–
        content = re.sub(r'\s+', ' ', content.strip())
        # ç§»é™¤æ—¶é—´æˆ³ç­‰å˜åŒ–å†…å®¹
        content = re.sub(r'\d{4}-\d{2}-\d{2}', '', content)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹
        content = re.sub(r'[^\w\s]', '', content)
        return content.lower()

    def is_cache_valid(self, concept_name: str, concept_content: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        cache_key = self._get_stable_cache_key(concept_name, concept_content)
        return cache_key in self.cache_data['embeddings']

    def get_cached_embedding(self, concept_name: str, concept_content: str) -> Optional[List[float]]:
        """è·å–ç¼“å­˜çš„åµŒå…¥å‘é‡"""
        cache_key = self._get_stable_cache_key(concept_name, concept_content)
        return self.cache_data['embeddings'].get(cache_key)

    def cache_embedding(self, concept_name: str, concept_content: str, embedding: List[float]):
        """ç¼“å­˜åµŒå…¥å‘é‡"""
        cache_key = self._get_stable_cache_key(concept_name, concept_content)
        self.cache_data['embeddings'][cache_key] = embedding
        self.cache_data['concept_hashes'][concept_name] = cache_key
        self.cache_data['metadata']['total_concepts'] = len(self.cache_data['concept_hashes'])
        self.cache_data['metadata']['last_updated'] = datetime.now().isoformat()
    
    # def save_embeddings_cache(self) -> None:
    #     """ä¿å­˜åµŒå…¥å‘é‡ç¼“å­˜"""
    #     try:
    #         cache_data = {
    #             'metadata': {
    #                 'total_embeddings': len(self.embeddings_cache),
    #                 'model': self.embedding_model,
    #                 'last_updated': self._get_current_timestamp()
    #             },
    #             'embeddings': self.embeddings_cache
    #         }
            
    #         with open(self.embeddings_cache_file, 'w', encoding='utf-8') as f:
    #             json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
    #         print(f"ğŸ’¾ å·²ä¿å­˜ {len(self.embeddings_cache)} ä¸ªBGEåµŒå…¥å‘é‡")
    #     except Exception as e:
    #         print(f"âŒ ä¿å­˜åµŒå…¥ç¼“å­˜å¤±è´¥: {e}")

    # ä¿å­˜åµŒå…¥ç¼“å­˜ - v2
    def save_embeddings_cache(self) -> None:
        """ä¿å­˜å¢å¼ºå‹åµŒå…¥å‘é‡ç¼“å­˜"""
        try:
            # æ›´æ–°å…ƒæ•°æ®
            self.cache_data['metadata']['last_updated'] = datetime.now().isoformat()
            self.cache_data['metadata']['total_concepts'] = len(self.cache_data['concept_hashes'])
            
            with open(self.embeddings_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å·²ä¿å­˜å¢å¼ºå‹ç¼“å­˜: {len(self.cache_data['embeddings'])} ä¸ªåµŒå…¥å‘é‡")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def get_embedding(self, text: str, concept_name: str = None) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬çš„BGEåµŒå…¥å‘é‡"""
        
        # å°è¯•ä»æ–‡æœ¬ä¸­æå–æ¦‚å¿µåç§°
        if not concept_name:
            concept_name = self._extract_concept_name_from_text(text)
        
        # æ£€æŸ¥å¢å¼ºå‹ç¼“å­˜
        if self.is_cache_valid(concept_name, text):
            cached_result = self.get_cached_embedding(concept_name, text)
            if cached_result:
                return cached_result
        
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "model": self.embedding_model,
                "input": text,
                "encoding_format": "float"
            }
            
            response = requests.post(self.embedding_url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            embedding = result['data'][0]['embedding']
            
            # ç¼“å­˜ç»“æœ
            self.cache_embedding(concept_name, text, embedding)
            return embedding
            
        except Exception as e:
            print(f"âŒ è·å–BGEåµŒå…¥å‘é‡å¤±è´¥: {e}")
            return None
        
    def _extract_concept_name_from_text(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–æ¦‚å¿µåç§°"""
        
        # å°è¯•ä»æ–‡æœ¬å¼€å¤´æå–ã€ç§‘ç›®ã€‘æ¦‚å¿µåæ ¼å¼
        title_match = re.search(r'ã€[^ã€‘]+ã€‘(.+?)(?:\n|$)', text)
        if title_match:
            return title_match.group(1).strip()
        
        # å°è¯•ä»titleå­—æ®µæå–
        title_field_match = re.search(r'title:\s*["\']?ã€[^ã€‘]+ã€‘(.+?)["\']?(?:\n|$)', text)
        if title_field_match:
            return title_field_match.group(1).strip()
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ–‡æœ¬çš„å‰20ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†
        clean_text = re.sub(r'[^\w\s]', '', text.strip())
        return clean_text[:20] if clean_text else "æœªçŸ¥æ¦‚å¿µ"
    
    def batch_get_embeddings(self, texts: List[str]) -> Dict[str, List[float]]:
        """æ‰¹é‡è·å–åµŒå…¥å‘é‡ï¼ˆæ›´é«˜æ•ˆï¼‰"""
        results = {}
        texts_to_process = []
        
        # æ£€æŸ¥ç¼“å­˜ï¼Œæ”¶é›†éœ€è¦å¤„ç†çš„æ–‡æœ¬
        for text in texts:
            concept_name = self._extract_concept_name_from_text(text)
            
            # æ£€æŸ¥å¢å¼ºå‹ç¼“å­˜
            if self.is_cache_valid(concept_name, text):
                cached_result = self.get_cached_embedding(concept_name, text)
                if cached_result:
                    results[text] = cached_result
                else:
                    texts_to_process.append(text)
            else:
                texts_to_process.append(text)
        
        if not texts_to_process:
            return results
        
        # æ‰¹é‡å¤„ç†ï¼ˆBGE-M3æ”¯æŒæ‰¹é‡è¾“å…¥ï¼‰
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
            batch_size = 10  # æ¯æ¬¡å¤„ç†10ä¸ª
            
            for i in range(0, len(texts_to_process), batch_size):
                batch_texts = texts_to_process[i:i + batch_size]
                
                data = {
                    "model": self.embedding_model,
                    "input": batch_texts,
                    "encoding_format": "float"
                }
                
                response = requests.post(self.embedding_url, headers=headers, json=data)
                response.raise_for_status()
                
                result = response.json()
                
                # å¤„ç†æ‰¹é‡ç»“æœ
                for j, embedding_data in enumerate(result['data']):
                    text = batch_texts[j]
                    embedding = embedding_data['embedding']
                    concept_name = self._extract_concept_name_from_text(text)
                    
                    # ç¼“å­˜å’Œè¿”å›ç»“æœ
                    self.cache_embedding(concept_name, text, embedding)
                    results[text] = embedding
                
                print(f"  ğŸ“Š å·²å¤„ç† {min(i + batch_size, len(texts_to_process))}/{len(texts_to_process)} ä¸ªæ¦‚å¿µ")
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
        
        return results
    
    def rerank_concepts(self, query: str, concept_docs: List[str], top_k: int = 20) -> List[Tuple[int, float]]:
        """ä½¿ç”¨BGE rerankerå¯¹æ¦‚å¿µè¿›è¡Œé‡æ’åº"""
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "model": self.rerank_model,
                "query": query,
                "documents": concept_docs,
                "return_documents": False
            }
            
            response = requests.post(self.rerank_url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            
            # æ£€æŸ¥å“åº”æ ¼å¼ - SiliconFlowä½¿ç”¨'results'è€Œä¸æ˜¯'data'
            if 'results' not in result:
                print(f"    âŒ APIå“åº”ä¸­ç¼ºå°‘'results'å­—æ®µ: {list(result.keys())}")
                return []
            
            # æå–æ’åºç»“æœ
            ranked_results = []
            for item in result['results']:
                if 'index' not in item or 'relevance_score' not in item:
                    print(f"    âš ï¸ æ’åºé¡¹ç¼ºå°‘å¿…è¦å­—æ®µ: {item}")
                    continue
                    
                index = item['index']
                score = item['relevance_score']  # SiliconFlowä½¿ç”¨relevance_score
                
                # SiliconFlowçš„åˆ†æ•°å·²ç»æ˜¯0-1èŒƒå›´ï¼Œä¸éœ€è¦sigmoidè½¬æ¢
                ranked_results.append((index, score))
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
            ranked_results.sort(key=lambda x: x[1], reverse=True)
            
            print(f"    ğŸ“Š é‡æ’åºå¾—åˆ†åˆ†å¸ƒ: æœ€é«˜{ranked_results[0][1]:.3f}, æœ€ä½{ranked_results[-1][1]:.3f}")
            
            return ranked_results[:top_k]
            
        except requests.exceptions.HTTPError as e:
            print(f"âŒ BGEé‡æ’åºHTTPé”™è¯¯: {e}")
            print(f"    å“åº”çŠ¶æ€ç : {response.status_code}")
            try:
                error_detail = response.json()
                print(f"    é”™è¯¯è¯¦æƒ…: {error_detail}")
            except:
                print(f"    å“åº”å†…å®¹: {response.text}")
            return []
        except requests.exceptions.RequestException as e:
            print(f"âŒ BGEé‡æ’åºè¯·æ±‚å¤±è´¥: {e}")
            return []
        except json.JSONDecodeError as e:
            print(f"âŒ BGEé‡æ’åºå“åº”è§£æå¤±è´¥: {e}")
            print(f"    å“åº”å†…å®¹: {response.text}")
            return []
        except Exception as e:
            print(f"âŒ BGEé‡æ’åºæœªçŸ¥é”™è¯¯: {e}")
            return []
    
    def build_concept_embeddings(self, force_rebuild: bool = False) -> None:
        """ä¸ºæ‰€æœ‰æ¦‚å¿µæ„å»ºBGEåµŒå…¥å‘é‡"""
        print("ğŸ” æ„å»ºæ¦‚å¿µBGEåµŒå…¥å‘é‡...")
        
        if not self.concept_manager.concept_database:
            print("âŒ æ¦‚å¿µæ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆæ‰«æç¬”è®°")
            return
        
        concepts_to_process = []
        
        for concept_name, concept_data in self.concept_manager.concept_database.items():
            # æ„å»ºæ¦‚å¿µæè¿°æ–‡æœ¬
            concept_content = self._build_concept_description(concept_name, concept_data)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
            if force_rebuild or not self.is_cache_valid(concept_name, concept_content):
                concepts_to_process.append((concept_name, concept_data, concept_content))
        
        if not concepts_to_process:
            print("âœ… æ‰€æœ‰æ¦‚å¿µçš„BGEåµŒå…¥å‘é‡å·²å­˜åœ¨")
            return
        
        print(f"ğŸ“ éœ€è¦å¤„ç† {len(concepts_to_process)} ä¸ªæ¦‚å¿µ")
        
        # å‡†å¤‡æ‰¹é‡æ–‡æœ¬
        concept_texts = []
        concept_names = []
        concept_contents = []
        
        for concept_name, concept_data, concept_content in concepts_to_process:
            concept_texts.append(concept_content)
            concept_names.append(concept_name)
            concept_contents.append(concept_content)
        
        # æ‰¹é‡è·å–åµŒå…¥å‘é‡
        print("ğŸš€ æ‰¹é‡è·å–BGEåµŒå…¥å‘é‡...")
        embeddings = self.batch_get_embeddings(concept_texts)
        
        # æ˜ å°„å›æ¦‚å¿µåç§°å¹¶ä½¿ç”¨æ–°ç¼“å­˜ç³»ç»Ÿ
        for i, concept_name in enumerate(concept_names):
            concept_text = concept_texts[i]
            concept_content = concept_contents[i]
            
            if concept_text in embeddings:
                # ä½¿ç”¨æ–°çš„ç¼“å­˜ç³»ç»Ÿ
                self.cache_embedding(concept_name, concept_content, embeddings[concept_text])
        
        # ä¿å­˜ç¼“å­˜
        self.save_embeddings_cache()
        print("âœ… æ¦‚å¿µBGEåµŒå…¥å‘é‡æ„å»ºå®Œæˆ")
    
    def _build_concept_description(self, concept_name: str, concept_data: Dict) -> str:
        """æ„å»ºæ¦‚å¿µçš„æè¿°æ–‡æœ¬ç”¨äºåµŒå…¥"""
        
        def remove_subject_prefix(text: str) -> str:
            """å»æ‰æ–‡æœ¬ä¸­çš„ã€ç§‘ç›®ã€‘å‰ç¼€"""
            if not isinstance(text, str):
                return ""
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»æ‰ã€xxxã€‘å‰ç¼€
            cleaned = re.sub(r'^ã€[^ã€‘]+ã€‘', '', text.strip())
            return cleaned.strip()
        
        parts = []
        
        # æ·»åŠ æ¦‚å¿µåç§°
        clean_concept_name = remove_subject_prefix(concept_name)
        if clean_concept_name:
            parts.append(clean_concept_name)
        
        # æ·»åŠ åˆ«å - ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ï¼Œå»æ‰ç§‘ç›®å‰ç¼€
        if concept_data.get('aliases'):
            aliases = concept_data['aliases']
            if isinstance(aliases, list):
                for alias in aliases:
                    if isinstance(alias, str) and alias.strip():
                        clean_alias = remove_subject_prefix(alias)
                        if clean_alias:
                            parts.append(clean_alias)
                    elif isinstance(alias, list):
                        for sub_alias in alias:
                            if isinstance(sub_alias, str) and sub_alias.strip():
                                clean_alias = remove_subject_prefix(sub_alias)
                                if clean_alias:
                                    parts.append(clean_alias)
            elif isinstance(aliases, str) and aliases.strip():
                clean_alias = remove_subject_prefix(aliases)
                if clean_alias:
                    parts.append(clean_alias)
        
        # æ·»åŠ ç§‘ç›®ä¿¡æ¯ï¼ˆä¿ç•™"æ³•è€ƒ"å‰ç¼€ç”¨äºä¸Šä¸‹æ–‡ï¼‰
        if concept_data.get('subject'):
            subject = concept_data['subject']
            if isinstance(subject, str) and subject.strip():
                parts.append(f"æ³•è€ƒ{subject.strip()}")
        
        # æ·»åŠ æ ‡ç­¾ - ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ï¼Œå»æ‰ç§‘ç›®å‰ç¼€
        if concept_data.get('tags'):
            tags = concept_data['tags']
            if isinstance(tags, list):
                for tag in tags:
                    if isinstance(tag, str) and tag.strip():
                        clean_tag = remove_subject_prefix(tag)
                        if clean_tag:
                            parts.append(clean_tag)
                    elif isinstance(tag, list):
                        for sub_tag in tag:
                            if isinstance(sub_tag, str) and sub_tag.strip():
                                clean_tag = remove_subject_prefix(sub_tag)
                                if clean_tag:
                                    parts.append(clean_tag)
            elif isinstance(tags, str) and tags.strip():
                clean_tag = remove_subject_prefix(tags)
                if clean_tag:
                    parts.append(clean_tag)
        
        # æ·»åŠ ç›¸å…³æ¦‚å¿µï¼ˆæœ‰é™æ•°é‡ï¼‰- ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ï¼Œå»æ‰ç§‘ç›®å‰ç¼€
        if concept_data.get('related_concepts'):
            related = concept_data['related_concepts']
            if isinstance(related, list):
                count = 0
                for concept in related:
                    if count >= 3:  # åªå–å‰3ä¸ª
                        break
                    if isinstance(concept, str) and concept.strip():
                        clean_concept = remove_subject_prefix(concept)
                        if clean_concept:
                            parts.append(clean_concept)
                            count += 1
                    elif isinstance(concept, list):
                        for sub_concept in concept:
                            if count >= 3:
                                break
                            if isinstance(sub_concept, str) and sub_concept.strip():
                                clean_concept = remove_subject_prefix(sub_concept)
                                if clean_concept:
                                    parts.append(clean_concept)
                                    count += 1
            elif isinstance(related, str) and related.strip():
                clean_concept = remove_subject_prefix(related)
                if clean_concept:
                    parts.append(clean_concept)
        
        return ' '.join(parts)
    
    def find_related_concepts_hybrid(
        self, 
        note_content: str, 
        note_title: str,
        embedding_top_k: int = 100,
        rerank_top_k: int = 15,
        rerank_threshold: float = 0.98  # è°ƒæ•´é»˜è®¤é˜ˆå€¼é€‚åº”BGEç‰¹æ€§
    ) -> List[Tuple[str, float]]:
        """
        æ··åˆæ£€ç´¢ï¼šå…ˆç”¨BGE embeddingå¬å›ï¼Œå†ç”¨rerankerç²¾æ’
        
        Args:
            note_content: ç¬”è®°å†…å®¹
            note_title: ç¬”è®°æ ‡é¢˜
            embedding_top_k: embeddingå¬å›çš„æ•°é‡
            rerank_top_k: rerankerç²¾æ’åè¿”å›çš„æ•°é‡
            rerank_threshold: rerankeråˆ†æ•°é˜ˆå€¼
            
        Returns:
            (æ¦‚å¿µå, rerankeråˆ†æ•°) çš„åˆ—è¡¨
        """
        # 1. æ„å»ºæŸ¥è¯¢æ–‡æœ¬
        query_text = f"{note_title} {note_content}"
        
        # 2. BGE embeddingå¬å›é˜¶æ®µ
        print(f"ğŸ” BGE embeddingå¬å› top-{embedding_top_k}...")
        
        query_embedding = self.get_embedding(query_text, concept_name="å…·ä½“æ¦‚å¿µå")
        if query_embedding is None:
            return []
        
        # è®¡ç®—ä¸æ‰€æœ‰æ¦‚å¿µçš„ä½™å¼¦ç›¸ä¼¼åº¦ - v2
        similarities = []
        for concept_name in self.concept_manager.concept_database.keys():
            if concept_name == note_title:  # è·³è¿‡è‡ªå·±
                continue
            
            # è·å–æ¦‚å¿µçš„æè¿°æ–‡æœ¬å’ŒåµŒå…¥å‘é‡
            concept_data = self.concept_manager.concept_database.get(concept_name, {})
            concept_content = self._build_concept_description(concept_name, concept_data)
            
            # æ£€æŸ¥ç¼“å­˜å¹¶è·å–åµŒå…¥å‘é‡
            if not self.is_cache_valid(concept_name, concept_content):
                continue
                
            concept_embedding = self.get_cached_embedding(concept_name, concept_content)
            if concept_embedding is None:
                continue
            
            similarity = self._cosine_similarity(query_embedding, concept_embedding)
            similarities.append((concept_name, similarity))

        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶å–top_k
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_concepts = similarities[:embedding_top_k]
        
        print(f"  ğŸ“Š å¬å› {len(top_concepts)} ä¸ªå€™é€‰æ¦‚å¿µ")
        
        if not top_concepts:
            return []
        
        # 3. BGE rerankerç²¾æ’é˜¶æ®µ
        print(f"ğŸ¯ BGE rerankerç²¾æ’ top-{rerank_top_k}...")
        
        # å‡†å¤‡rerankerè¾“å…¥
        concept_names = [concept for concept, _ in top_concepts]
        concept_docs = []
        
        for concept_name in concept_names:
            concept_data = self.concept_manager.concept_database.get(concept_name, {})
            concept_doc = self._build_concept_description(concept_name, concept_data)
            concept_docs.append(concept_doc)
        
        # è°ƒç”¨reranker
        rerank_results = self.rerank_concepts(query_text, concept_docs, rerank_top_k)
        
        # æ˜ å°„å›æ¦‚å¿µåç§°å¹¶è¿‡æ»¤
        final_results = []
        for doc_index, rerank_score in rerank_results:
            if rerank_score >= rerank_threshold:
                concept_name = concept_names[doc_index]
                final_results.append((concept_name, rerank_score))
        
        # å¦‚æœé˜ˆå€¼è¿‡é«˜å¯¼è‡´ç»“æœå¤ªå°‘ï¼ŒåŠ¨æ€è°ƒæ•´
        if len(final_results) < 3 and rerank_results:
            # å–å‰5ä¸ªæœ€é«˜åˆ†çš„ï¼Œä¸ç®¡é˜ˆå€¼
            print(f"    ğŸ”„ é˜ˆå€¼{rerank_threshold}è¿‡é«˜ï¼ŒåŠ¨æ€è°ƒæ•´ä¸ºå‰5ä¸ªæœ€é«˜åˆ†æ¦‚å¿µ")
            final_results = []
            for doc_index, rerank_score in rerank_results[:5]:
                concept_name = concept_names[doc_index]
                final_results.append((concept_name, rerank_score))
        
        print(f"  âœ¨ ç²¾æ’åä¿ç•™ {len(final_results)} ä¸ªé«˜è´¨é‡æ¦‚å¿µ")
        
        # æ˜¾ç¤ºtop 5ç»“æœ
        for i, (concept, score) in enumerate(final_results[:5]):
            print(f"    {i+1}. {concept}: {score:.4f}")  # æé«˜ç²¾åº¦æ˜¾ç¤º
        
        return final_results
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            import numpy as np
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except Exception:
            return 0.0
    
    def enhance_note_with_hybrid_search(
        self, 
        note_content: str, 
        note_title: str,
        embedding_top_k: int = 100,
        rerank_top_k: int = 15,
        rerank_threshold: float = 0.98  # è°ƒæ•´é»˜è®¤é˜ˆå€¼
    ) -> Optional[Dict]:
        """
        ä½¿ç”¨æ··åˆæ£€ç´¢å¢å¼ºå•ä¸ªç¬”è®°çš„æ¦‚å¿µå…³ç³»
        """
        print(f"ğŸ” æ··åˆæ£€ç´¢æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µ...")
        
        # 1. æ··åˆæ£€ç´¢æ‰¾åˆ°æœ€ç›¸å…³çš„æ¦‚å¿µ
        related_concepts = self.find_related_concepts_hybrid(
            note_content, note_title, embedding_top_k, rerank_top_k, rerank_threshold
        )
        
        if not related_concepts:
            print(f"  âš ï¸ æœªæ‰¾åˆ°é‡æ’åºåˆ†æ•°é«˜äº {rerank_threshold} çš„ç›¸å…³æ¦‚å¿µ")
            return {'modified': False}
        
        # 2. æ„å»ºç²¾ç®€çš„æ¦‚å¿µåˆ—è¡¨ç»™AI
        filtered_concepts = {
            'existing_concepts': [concept for concept, _ in related_concepts],
            'concept_aliases': {},
            'concept_relationships': {},
            'rerank_scores': {concept: score for concept, score in related_concepts}
        }
        
        # æ·»åŠ æ¦‚å¿µçš„è¯¦ç»†ä¿¡æ¯
        for concept_name, _ in related_concepts:
            if concept_name in self.concept_manager.concept_database:
                data = self.concept_manager.concept_database[concept_name]
                filtered_concepts['concept_aliases'][concept_name] = data.get('aliases', [])
                filtered_concepts['concept_relationships'][concept_name] = data.get('related_concepts', [])
        
        # 3. è°ƒç”¨AIè¿›è¡Œç²¾ç¡®åˆ†æ
        print(f"ğŸ¤– AIåˆ†ææœ€ç›¸å…³çš„ {len(related_concepts)} ä¸ªæ¦‚å¿µ...")
        
        enhanced_prompt = self._build_hybrid_enhanced_prompt(
            note_content, note_title, filtered_concepts
        )
        
        try:
            response = self.ai_processor.client.chat.completions.create(
                model=self.ai_processor.model,
                messages=[{"role": "user", "content": enhanced_prompt}],
                temperature=0,
            )
            
            result = self.ai_processor._parse_single_note_enhancement_response(
                response.choices[0].message.content, 
                note_content
            )
            
            if result and result.get('modified'):
                print(f"  âœ… AIå»ºè®®ä¿®æ”¹")
            else:
                print(f"  âš ï¸ AIè®¤ä¸ºæ— éœ€ä¿®æ”¹")
            
            return result
            
        except Exception as e:
            print(f"âŒ AIå¢å¼ºå¤±è´¥: {e}")
            return {'modified': False}
    
    def _build_hybrid_enhanced_prompt(
        self, 
        note_content: str, 
        note_title: str, 
        filtered_concepts: Dict
    ) -> str:
        """æ„å»ºåŸºäºæ··åˆæ£€ç´¢çš„å¢å¼ºæç¤ºè¯"""
        
        concepts_info = []
        for concept in filtered_concepts['existing_concepts']:
            aliases = filtered_concepts['concept_aliases'].get(concept, [])
            score = filtered_concepts['rerank_scores'].get(concept, 0.0)
            
            info = f"- {concept} (ç›¸å…³åº¦: {score:.3f})"
            if aliases:
                info += f" [åˆ«å: {', '.join(aliases[:2])}]"
            concepts_info.append(info)
        
        concepts_list = '\n'.join(concepts_info)
        
        return f"""
(defun æ³•è€ƒæ¦‚å¿µé“¾æ¥ä¸“å®¶ ()
  "åŸºäºæ¦‚å¿µåº“ä¼˜åŒ–æ³•è€ƒç¬”è®°æ¦‚å¿µå…³ç³»"
  (èº«ä»½ . æ³•è€ƒçŸ¥è¯†ä½“ç³»ä¸“å®¶)
  (ä¸“é•¿ . (æ¦‚å¿µå…³ç³»åˆ†æ çŸ¥è¯†å›¾è°±æ„å»º è¯­ä¹‰ç›¸å…³æ€§åˆ¤æ–­))
  (å·¥å…·ç®± . (æ¦‚å¿µåº“åŒ¹é… åŒé“¾è¯­æ³•è§„èŒƒ å…³è”æ€§åˆ†æ))
  (è´¨é‡æ ‡å‡† . (é«˜åº¦ç›¸å…³æ€§ ä¸¥æ ¼æŠŠæ§ æ¦‚å¿µåº“å‡†ç¡®æ€§))
  (é“¾æ¥æ ¼å¼ . "[[ã€ç§‘ç›®ã€‘æ¦‚å¿µå|æ¦‚å¿µå/åˆ«å]]"))

(defun æ¦‚å¿µå…³ç³»åˆ†æå™¨ (ç¬”è®°æ ‡é¢˜ ç¬”è®°å†…å®¹ æ¦‚å¿µåº“)
  "åˆ†æç¬”è®°ä¸æ¦‚å¿µåº“çš„å…³è”æ€§ï¼Œä¼˜åŒ–æ¦‚å¿µé“¾æ¥"
  (let* ((å†…å®¹è§£æ (è¯­ä¹‰åˆ†æ ç¬”è®°å†…å®¹))
         (ç°æœ‰é“¾æ¥æ£€æŸ¥ (éªŒè¯ç°æœ‰é“¾æ¥ ç¬”è®°å†…å®¹ æ¦‚å¿µåº“))
         (é—æ¼æ¦‚å¿µè¯†åˆ« (å‘ç°ç›¸å…³æ¦‚å¿µ å†…å®¹è§£æ æ¦‚å¿µåº“))
         (å…³ç³»åˆ†ç±» (æ¦‚å¿µåˆ†ç±»å™¨ é—æ¼æ¦‚å¿µè¯†åˆ«))
         (é“¾æ¥ä¼˜åŒ– (åŒé“¾æ ¼å¼åŒ– å…³ç³»åˆ†ç±» ç°æœ‰é“¾æ¥æ£€æŸ¥)))
    (é™é»˜è¾“å‡º é“¾æ¥ä¼˜åŒ–)))

(defun éªŒè¯ç°æœ‰é“¾æ¥ (ç¬”è®°å†…å®¹ æ¦‚å¿µåº“)
  "æ£€æŸ¥ç¬”è®°ä¸­ç°æœ‰çš„[[æ¦‚å¿µ]]é“¾æ¥æ˜¯å¦å‡†ç¡®"
  (let ((ç°æœ‰é“¾æ¥ (æå–ç°æœ‰é“¾æ¥ ç¬”è®°å†…å®¹)))
    (filter (lambda (é“¾æ¥)
              (æ¦‚å¿µåº“ä¸­å­˜åœ¨? é“¾æ¥ æ¦‚å¿µåº“))
            ç°æœ‰é“¾æ¥)))

(defun å‘ç°ç›¸å…³æ¦‚å¿µ (ç¬”è®°è¯­ä¹‰ æ¦‚å¿µåº“)
  "è¯†åˆ«ç¬”è®°ä¸­å¯èƒ½é—æ¼çš„é‡è¦æ¦‚å¿µå…³è”"
  (filter (lambda (æ¦‚å¿µ)
            (and (æ¦‚å¿µåº“ä¸­å­˜åœ¨? æ¦‚å¿µ æ¦‚å¿µåº“)
                 (or (ç›´æ¥æåŠ? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰)
                     (è¯­ä¹‰ç›¸å…³? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰)
                     (ä¸Šä¸‹çº§å…³ç³»? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰)
                     (å¹¶åˆ—å…³ç³»? æ¦‚å¿µ ç¬”è®°è¯­ä¹‰))))
          æ¦‚å¿µåº“))

(defun æ¦‚å¿µåˆ†ç±»å™¨ (ç›¸å…³æ¦‚å¿µ)
  "æŒ‰ç…§å…³ç³»ç±»å‹å¯¹æ¦‚å¿µè¿›è¡Œåˆ†ç±»"
  (let ((åˆ†ç±»ç»“æœ '()))
    (dolist (æ¦‚å¿µ ç›¸å…³æ¦‚å¿µ)
      (cond ((æ ¸å¿ƒç›¸å…³? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :æ ¸å¿ƒ)))
            ((æ„æˆè¦ä»¶ç›¸å…³? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :è¦ä»¶)))
            ((æ‰¿æ‹…æ–¹å¼ç›¸å…³? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :æ–¹å¼)))
            ((å¯¹æ¯”æ¦‚å¿µ? æ¦‚å¿µ) (push æ¦‚å¿µ (getf åˆ†ç±»ç»“æœ :å¯¹æ¯”)))))
    åˆ†ç±»ç»“æœ))

(defun é™é»˜è¾“å‡º (ä¼˜åŒ–ç»“æœ)
  "é™é»˜æ‰§è¡Œï¼Œä»…è¾“å‡ºæ ‡å‡†æ ¼å¼ç»“æœï¼Œç¦æ­¢ä»»ä½•ä»£ç å—åŒ…è£¹"
  (cond ((éœ€è¦ä¿®æ”¹? ä¼˜åŒ–ç»“æœ)
         (format nil "MODIFIED: true~%ENHANCED_CONTENT:~%~a" 
                 (ç›´æ¥è¾“å‡ºæ–‡å­—å†…å®¹ ä¼˜åŒ–ç»“æœ)))
        (t "MODIFIED: false")))

(defun ç›´æ¥è¾“å‡ºæ–‡å­—å†…å®¹ (ä¼˜åŒ–ç»“æœ)
  "ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬å†…å®¹ï¼Œä¸ä½¿ç”¨ä»»ä½•markdownä»£ç å—åŒ…è£¹"
  (ç”Ÿæˆå®Œæ•´å†…å®¹ ä¼˜åŒ–ç»“æœ))

(defun é™é»˜æ‰§è¡Œæ¨¡å¼ ()
  "è®¾ç½®é™é»˜æ‰§è¡Œçº¦æŸ"
  '((ç¦æ­¢è§£é‡Š . t)
    (ç¦æ­¢åˆ†æè¿‡ç¨‹è¾“å‡º . t) 
    (ç¦æ­¢é¢å¤–è¯´æ˜ . t)
    (ä»…è¾“å‡ºç»“æœ . t)
    (ä¸¥æ ¼æ ¼å¼æ§åˆ¶ . t)
    (ç¦æ­¢ä»£ç å—åŒ…è£¹ . t)))

(defun è´¨é‡æ§åˆ¶è§„åˆ™ ()
  "ç¡®ä¿è¾“å‡ºè´¨é‡çš„è§„åˆ™é›†"
  '((æ¦‚å¿µåº“å­˜åœ¨æ€§ . "åªæ·»åŠ ç¡®å®å­˜åœ¨äºæ¦‚å¿µåº“ä¸­çš„æ¦‚å¿µé“¾æ¥")
    (é«˜åº¦ç›¸å…³æ€§ . "ç¡®ä¿æ–°å¢çš„æ¦‚å¿µé“¾æ¥ä¸ç¬”è®°å†…å®¹é«˜åº¦ç›¸å…³")
    (ç°æœ‰é“¾æ¥éªŒè¯ . "æ£€æŸ¥ç¬”è®°ä¸­ç°æœ‰çš„[[æ¦‚å¿µ]]é“¾æ¥æ˜¯å¦å‡†ç¡®")
    (æ ¼å¼æ ‡å‡† . "ä¸¥æ ¼ä½¿ç”¨åŒé“¾æ˜¾ç¤ºåˆ«åæ ¼å¼")
    (ç§»é™¤æ— å…³ . "ç§»é™¤æŒ‡å‘ä¸å­˜åœ¨æ¦‚å¿µçš„é“¾æ¥")))

(defun start ()
  "å¯åŠ¨é™é»˜æ‰§è¡Œæ¨¡å¼"
  (setq system-role æ³•è€ƒæ¦‚å¿µé“¾æ¥ä¸“å®¶)
  (setq execution-mode (é™é»˜æ‰§è¡Œæ¨¡å¼))
  (setq quality-rules (è´¨é‡æ§åˆ¶è§„åˆ™))
  (setq output-only t))

;; æ‰§è¡Œçº¦æŸ
;; 1. å¿…é¡»å¯åŠ¨é™é»˜æ‰§è¡Œæ¨¡å¼
;; 2. ç¦æ­¢è¾“å‡ºä»»ä½•åˆ†æè¿‡ç¨‹ã€è§£é‡Šæˆ–è¯´æ˜
;; 3. ç¦æ­¢è¾“å‡ºæ€è€ƒè¿‡ç¨‹æˆ–ä¸­é—´ç»“æœ
;; 4. ä¸¥æ ¼è¾“å‡ºæ ¼å¼ï¼šä»… "MODIFIED: true/false" å’Œå¯¹åº”å†…å®¹
;; 5. ä»»ä½•éæ ‡å‡†æ ¼å¼çš„è¾“å‡ºéƒ½æ˜¯é”™è¯¯çš„
;; 6. ä¸å¾—æ·»åŠ å¼•è¨€ã€æ€»ç»“ã€è§£é‡Šæˆ–å…¶ä»–æ–‡å­—
;; 7. ç¦æ­¢ä½¿ç”¨```ä»£ç å—åŒ…è£¹è¾“å‡ºå†…å®¹

;; ========== Pythonè„šæœ¬æ¨¡æ¿æ¥å£ ==========

è¯·åˆ†æä»¥ä¸‹ç¬”è®°ï¼Œæ£€æŸ¥å¹¶ä¼˜åŒ–å…¶æ¦‚å¿µå…³ç³»é“¾æ¥ï¼š

**ç¬”è®°æ ‡é¢˜ï¼š**{note_title}

**ç¬”è®°å†…å®¹ï¼š**
{note_content}

**ç°æœ‰æ¦‚å¿µåº“ï¼š**
{concepts_list}

æ‰§è¡Œåˆ†æä»»åŠ¡ï¼š(æ¦‚å¿µå…³ç³»åˆ†æå™¨ "{note_title}" ç¬”è®°å†…å®¹ æ¦‚å¿µåº“)"""
    
    def batch_enhance_with_hybrid_search(
        self, 
        notes: List[Dict[str, Any]],
        rebuild_embeddings: bool = False,
        embedding_top_k: int = 100,
        rerank_top_k: int = 15,
        rerank_threshold: float = 0.98,
        force_full_rebuild: bool = False  # æ–°å¢å‚æ•°
    ) -> Dict[str, int]:
        """ä½¿ç”¨æ··åˆæ£€ç´¢æ‰¹é‡å¢å¼ºç¬”è®°"""
        print("ğŸš€ å¯åŠ¨åŸºäºBGEæ··åˆæ£€ç´¢çš„ç¬”è®°å¢å¼º...")
        
        # 1. ç¡®ä¿åµŒå…¥å‘é‡å·²æ„å»º
        self.build_concept_embeddings(force_rebuild=rebuild_embeddings)
        
        # 2. æ™ºèƒ½å¢é‡æ£€æµ‹
        if force_full_rebuild:
            print("ğŸ”„ å¼ºåˆ¶å®Œæ•´é‡å»ºæ¨¡å¼")
            notes_to_process = notes
        else:
            print("ğŸ¯ æ™ºèƒ½å¢é‡æ£€æµ‹...")
            notes_to_process = self.incremental_processor.get_notes_needing_enhancement(
                notes, self.concept_manager
            )
            
            if not notes_to_process:
                return {'total': len(notes), 'enhanced': 0, 'unchanged': len(notes), 'failed': 0}
            
            print(f"ğŸ“ å¢é‡æ¨¡å¼: éœ€è¦å¤„ç† {len(notes_to_process)}/{len(notes)} ä¸ªç¬”è®°")
        
        # 3. å¤„ç†ç¬”è®°ï¼ˆåŸæœ‰é€»è¾‘ä¿æŒä¸å˜ï¼‰
        stats = {
            'total': len(notes),
            'enhanced': 0,
            'unchanged': len(notes) - len(notes_to_process),  # æœªå¤„ç†çš„è§†ä¸ºæœªå˜åŒ–
            'failed': 0
        }
        
        for i, note_info in enumerate(notes_to_process, 1):
            print(f"\nğŸ”„ å¤„ç† {i}/{len(notes_to_process)}: {note_info['title']}")
            
            try:
                result = self.enhance_note_with_hybrid_search(
                    note_info['content'], 
                    note_info['title'],
                    embedding_top_k,
                    rerank_top_k,
                    rerank_threshold
                )
                
                if result and result.get('modified'):
                    if self._apply_enhancement(note_info, result):
                        stats['enhanced'] += 1
                        print(f"  âœ… å¢å¼ºæˆåŠŸ")
                    else:
                        stats['failed'] += 1
                        print(f"  âŒ åº”ç”¨ä¿®æ”¹å¤±è´¥")
                else:
                    stats['unchanged'] += 1
                    print(f"  âš ï¸ æ— éœ€ä¿®æ”¹")
                    
            except Exception as e:
                stats['failed'] += 1
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
        
        # 4. æ›´æ–°å¢é‡è¿½è¸ª
        self.incremental_processor.update_tracking_after_enhancement(
            notes_to_process, self.concept_manager
        )
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        print(f"\nğŸ‰ åŸºäºBGEæ··åˆæ£€ç´¢çš„æ‰¹é‡å¢å¼ºå®Œæˆï¼")
        print(f"  ğŸ“Š æ€»è®¡: {stats['total']} ä¸ªç¬”è®°")
        print(f"  âœ… æˆåŠŸå¢å¼º: {stats['enhanced']} ä¸ª")
        print(f"  âš ï¸ æ— éœ€ä¿®æ”¹: {stats['unchanged']} ä¸ª")
        print(f"  âŒ å¤„ç†å¤±è´¥: {stats['failed']} ä¸ª")
        
        return stats
    
    def _apply_enhancement(self, note_info: Dict[str, Any], result: Dict) -> bool:
        """åº”ç”¨å¢å¼ºç»“æœåˆ°æ–‡ä»¶"""
        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_path = note_info['file_path'] + '.backup'
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(note_info['content'])
            
            # å†™å…¥å¢å¼ºåçš„å†…å®¹
            with open(note_info['file_path'], 'w', encoding='utf-8') as f:
                f.write(result['enhanced_content'])
            
            # åˆ é™¤å¤‡ä»½æ–‡ä»¶
            os.remove(backup_path)
            return True
            
        except Exception as e:
            print(f"âŒ åº”ç”¨å¢å¼ºå¤±è´¥: {e}")
            return False
    
    def _get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


# config.py ä¸­éœ€è¦æ·»åŠ SiliconFlowé…ç½®
class Config:
    # ç°æœ‰é…ç½®...
    
    # SiliconFlow APIé…ç½®
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY", "your_siliconflow_api_key")


# ä¸»ç¨‹åºé›†æˆä»£ç ç‰‡æ®µ
def integrate_siliconflow_enhancer():
    """é›†æˆSiliconFlowå¢å¼ºå™¨åˆ°ä¸»ç¨‹åºçš„ç¤ºä¾‹ä»£ç """
    
    # åœ¨ main.py çš„ LawExamNoteProcessor ç±»ä¸­ä¿®æ”¹ï¼š
    
    def __init__(self):
        # ... ç°æœ‰åˆå§‹åŒ–ä»£ç  ...
        
        # æ·»åŠ SiliconFlowå¢å¼ºå™¨
        self.siliconflow_enhancer = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    def _get_siliconflow_enhancer(self):
        """è·å–SiliconFlowå¢å¼ºå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self.siliconflow_enhancer is None:
            from siliconflow_concept_enhancer import SiliconFlowConceptEnhancer
            self.siliconflow_enhancer = SiliconFlowConceptEnhancer(
                Config.SILICONFLOW_API_KEY,
                self.ai_processor, 
                self.concept_manager
            )
        return self.siliconflow_enhancer
    
    def _enhance_with_hybrid_search(self):
        """ä½¿ç”¨BGEæ··åˆæ£€ç´¢å¢å¼ºç¬”è®°"""
        print("\nğŸ”– BGEæ··åˆæ£€ç´¢æ¨¡å¼ï¼ˆembeddingå¬å›+rerankerç²¾æ’ï¼‰")
        
        enhancer = self._get_siliconflow_enhancer()
        
        print("å‚æ•°é…ç½®:")
        print("1. ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆå¬å›100ä¸ªï¼Œç²¾æ’15ä¸ªï¼Œé˜ˆå€¼0.3ï¼‰")
        print("2. è‡ªå®šä¹‰å‚æ•°")
        print("3. è¿”å›")
        
        config_choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        if config_choice == '2':
            try:
                embedding_top_k = int(input("embeddingå¬å›æ•°é‡ (å»ºè®®50-200): ") or "100")
                rerank_top_k = int(input("rerankerç²¾æ’æ•°é‡ (å»ºè®®10-20): ") or "15")
                rerank_threshold = float(input("rerankeråˆ†æ•°é˜ˆå€¼ (å»ºè®®0.2-0.5): ") or "0.3")
            except ValueError:
                print("âŒ å‚æ•°æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                embedding_top_k, rerank_top_k, rerank_threshold = 100, 15, 0.3
        elif config_choice == '1':
            embedding_top_k, rerank_top_k, rerank_threshold = 100, 15, 0.3
        else:
            return
        
        print("1. å¢å¼ºæ‰€æœ‰ç§‘ç›®çš„ç¬”è®°")
        print("2. å¢å¼ºç‰¹å®šç§‘ç›®çš„ç¬”è®°")
        print("3. è¿”å›")
        
        choice = input("è¯·é€‰æ‹©æ“ä½œ (1-3): ").strip()
        
        if choice == '1':
            notes = self._collect_all_law_notes()
        if notes:
            # è¿‡æ»¤æ‰é”™é¢˜æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
            filtered_notes = []
            for note in notes:
                if 'é”™é¢˜' not in note.get('file_path', ''):
                    filtered_notes.append(note)
                else:
                    print(f"è·³è¿‡é”™é¢˜æ–‡ä»¶: {os.path.basename(note.get('file_path', ''))}")
            
            notes = filtered_notes
            enhancer.batch_enhance_with_hybrid_search(
                notes, False, embedding_top_k, rerank_top_k, rerank_threshold
            )
        elif choice == '2':
            subject = self._select_subject()
            if subject:
                notes = self._collect_subject_notes_by_name(subject)
                if notes:
                    enhancer.batch_enhance_with_hybrid_search(
                        notes, False, embedding_top_k, rerank_top_k, rerank_threshold
                    )
        
        # é‡æ–°æ‰«ææ›´æ–°æ¦‚å¿µæ•°æ®åº“
        if choice in ['1', '2']:
            print(f"\nğŸ“š é‡æ–°æ‰«ææ›´æ–°æ¦‚å¿µæ•°æ®åº“...")
            self.concept_manager.scan_existing_notes()
