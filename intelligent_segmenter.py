# intelligent_segmenter.py - ä¿®æ”¹ç‰ˆæœ¬ï¼šæ”¯æŒæŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µ
import re
import json
from typing import List, Dict, Any, Optional, NamedTuple
from dataclasses import dataclass

# æ—¶é—´èŒƒå›´ç±»ï¼ˆä¿æŒä¸å˜ï¼‰
@dataclass
class TimeRange:
    start: float
    end: float
    kp_ids: List[str]  # å…³è”çš„çŸ¥è¯†ç‚¹IDåˆ—è¡¨
    
    @property
    def duration(self) -> float:
        return self.end - self.start

# åˆ†æ®µç±»ï¼ˆä¿æŒä¸å˜ï¼‰
@dataclass
class Segment:
    text: str
    time_range: TimeRange
    knowledge_points: List[str]
    token_count: int
    buffer_info: Dict[str, Any] = None

class TimeParser:
    """æ—¶é—´è§£æå·¥å…·ç±»ï¼ˆä¿æŒä¸å˜ï¼‰"""
    
    @staticmethod
    def parse_time_to_seconds(time_str: str) -> Optional[float]:
        """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°"""
        if not time_str:
            return None
        
        # ç§»é™¤å¯èƒ½çš„æ–¹æ‹¬å·
        time_str = time_str.strip('[]')
        
        # åŒ¹é…ä¸åŒçš„æ—¶é—´æ ¼å¼
        patterns = [
            r'^(\d{1,2}):(\d{2})\.(\d{1,3})$',  # MM:SS.mmm
            r'^(\d{1,2}):(\d{2}):(\d{2})\.(\d{1,3})$',  # HH:MM:SS.mmm
            r'^(\d{1,2}):(\d{2}):(\d{2}),(\d{1,3})$',  # HH:MM:SS,mmm (SRTæ ¼å¼)
            r'^(\d{1,2}):(\d{2})$',  # MM:SS
            r'^(\d{1,2}):(\d{2}):(\d{2})$',  # HH:MM:SS
        ]
        
        for pattern in patterns:
            match = re.match(pattern, time_str)
            if match:
                groups = match.groups()
                if len(groups) == 3:  # MM:SS.mmm
                    minutes, seconds, milliseconds = groups
                    return int(minutes) * 60 + int(seconds) + int(milliseconds) / 1000
                elif len(groups) == 4:  # HH:MM:SS.mmm
                    hours, minutes, seconds, milliseconds = groups
                    return int(hours) * 3600 + int(minutes) * 60 + int(seconds) + int(milliseconds) / 1000
                elif len(groups) == 2:  # MM:SS
                    minutes, seconds = groups
                    return int(minutes) * 60 + int(seconds)
        
        return None
    
    @staticmethod
    def parse_time_range(time_range_str: str, kp_id: str) -> Optional[TimeRange]:
        """è§£ææ—¶é—´èŒƒå›´å­—ç¬¦ä¸²"""
        if not time_range_str:
            return None
        
        # åŒ¹é…æ—¶é—´èŒƒå›´æ ¼å¼ï¼šstart-end æˆ– start--end
        range_match = re.search(r'(.+?)-{1,2}(.+)', time_range_str)
        if not range_match:
            return None
        
        start_str = range_match.group(1).strip()
        end_str = range_match.group(2).strip()
        
        start_seconds = TimeParser.parse_time_to_seconds(start_str)
        end_seconds = TimeParser.parse_time_to_seconds(end_str)
        
        if start_seconds is not None and end_seconds is not None:
            # ç¡®ä¿å¼€å§‹æ—¶é—´å°äºç»“æŸæ—¶é—´
            if start_seconds > end_seconds:
                start_seconds, end_seconds = end_seconds, start_seconds
            
            return TimeRange(start_seconds, end_seconds, [kp_id])
        
        return None

class SubtitleParser:
    """å­—å¹•è§£æå™¨ï¼ˆä¿æŒä¸å˜ï¼‰"""
    
    @staticmethod
    def parse_lrc_content(content: str) -> List[Dict]:
        """è§£æLRCæ ¼å¼å­—å¹•"""
        lines = []
        lrc_pattern = r'\[(\d{1,2}:\d{2}(?:\.\d{1,3})?)\](.+)'
        
        for line in content.split('\n'):
            line = line.strip()
            if not line:
                continue
                
            match = re.search(lrc_pattern, line)
            if match:
                time_str = match.group(1)
                text = match.group(2).strip()
                
                timestamp = TimeParser.parse_time_to_seconds(time_str)
                if timestamp is not None:
                    lines.append({
                        'timestamp': timestamp,
                        'text': text,
                        'original_time': time_str
                    })
        
        return lines
    
    @staticmethod
    def parse_srt_content(content: str) -> List[Dict]:
        """è§£æSRTæ ¼å¼å­—å¹•"""
        lines = []
        blocks = content.split('\n\n')
        
        for block in blocks:
            block = block.strip()
            if not block:
                continue
            
            block_lines = block.split('\n')
            if len(block_lines) < 3:
                continue
            
            # è§£ææ—¶é—´è¡Œ
            time_line = block_lines[1]
            time_match = re.search(r'(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})', time_line)
            if not time_match:
                continue
            
            start_time_str = time_match.group(1).replace(',', '.')
            end_time_str = time_match.group(2).replace(',', '.')
            
            start_timestamp = TimeParser.parse_time_to_seconds(start_time_str)
            end_timestamp = TimeParser.parse_time_to_seconds(end_time_str)
            
            if start_timestamp is not None and end_timestamp is not None:
                # åˆå¹¶æ–‡æœ¬è¡Œ
                text = ' '.join(block_lines[2:]).strip()
                
                lines.append({
                    'timestamp': start_timestamp,
                    'end_timestamp': end_timestamp,
                    'text': text,
                    'original_time': f"{time_match.group(1)}-{time_match.group(2)}"
                })
        
        return lines
    
    @staticmethod
    def parse_txt_content(content: str) -> List[Dict]:
        """è§£ææ™®é€šæ–‡æœ¬ï¼ˆæŒ‰è¡Œå¤„ç†ï¼‰"""
        lines = []
        for i, line in enumerate(content.split('\n')):
            line = line.strip()
            if line:
                lines.append({
                    'timestamp': i,  # ä½¿ç”¨è¡Œå·ä½œä¸ºè™šæ‹Ÿæ—¶é—´æˆ³
                    'text': line,
                    'original_time': f"line_{i+1}"
                })
        
        return lines
    
    @classmethod
    def parse_subtitle_content(cls, content: str, file_format: str = 'auto') -> List[Dict]:
        """è§£æå­—å¹•å†…å®¹"""
        if file_format == 'auto':
            # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
            if re.search(r'\[\d{1,2}:\d{2}(?:\.\d{1,3})?\]', content):
                file_format = 'lrc'
            elif re.search(r'\d{2}:\d{2}:\d{2},\d{3}\s*-->\s*\d{2}:\d{2}:\d{2},\d{3}', content):
                file_format = 'srt'
            else:
                file_format = 'txt'
        
        # æ ¹æ®æ ¼å¼è§£æ
        if file_format == 'lrc':
            return cls.parse_lrc_content(content)
        elif file_format == 'srt':
            return cls.parse_srt_content(content)
        else:  # txt æˆ–å…¶ä»–
            return cls.parse_txt_content(content)


class IntelligentSegmenter:
    """æ™ºèƒ½åˆ†æ®µå¤„ç†å™¨ - ä¿®æ”¹ç‰ˆæœ¬ï¼šæ”¯æŒæŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µ"""
    
    def __init__(self, buffer_seconds: float = 30.0, max_gap_seconds: float = 5.0):
        """
        åˆå§‹åŒ–åˆ†æ®µå™¨
        
        Args:
            buffer_seconds: ç¼“å†²åŒºå¤§å°ï¼ˆç§’ï¼‰
            max_gap_seconds: æœ€å¤§é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå°äºæ­¤å€¼çš„æ—¶é—´æ®µå°†åˆå¹¶
        """
        self.buffer_seconds = buffer_seconds
        self.max_gap_seconds = max_gap_seconds
        self.subtitle_parser = SubtitleParser()
    
    def segment_by_knowledge_points(self, subtitle_content: str, analysis_result: Dict[str, Any],
                                  file_format: str = 'auto') -> List[Segment]:
        """
        æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µï¼ˆæ–°æ–¹æ³•ï¼‰
        
        Args:
            subtitle_content: å­—å¹•æ–‡ä»¶å†…å®¹
            analysis_result: ç¬¬ä¸€æ­¥åˆ†æç»“æœ
            file_format: å­—å¹•æ–‡ä»¶æ ¼å¼
            
        Returns:
            æ¯ä¸ªçŸ¥è¯†ç‚¹å¯¹åº”ä¸€ä¸ªåˆ†æ®µçš„ç»“æœåˆ—è¡¨
        """
        try:
            print(f"ğŸ”§ æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µå¤„ç† (æ ¼å¼: {file_format})")
            
            # 1. è§£æå­—å¹•å†…å®¹
            subtitle_lines = self.subtitle_parser.parse_subtitle_content(subtitle_content, file_format)
            
            if not subtitle_lines:
                print("âš ï¸ å­—å¹•å†…å®¹è§£æä¸ºç©ºï¼Œå°†ä½¿ç”¨å®Œæ•´å†…å®¹ä½œä¸ºfallback")
                return self._create_individual_fallback_segments(subtitle_content, analysis_result)
            
            print(f"âœ… è§£æåˆ° {len(subtitle_lines)} è¡Œå­—å¹•")
            
            # 2. ä»åˆ†æç»“æœä¸­æå–çŸ¥è¯†ç‚¹æ—¶é—´èŒƒå›´
            print("ğŸ” æå–å„çŸ¥è¯†ç‚¹çš„æ—¶é—´èŒƒå›´")
            individual_ranges = self._extract_individual_knowledge_point_ranges(analysis_result)
            
            if not individual_ranges:
                print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´èŒƒå›´ï¼Œå°†ä½¿ç”¨å®Œæ•´å†…å®¹")
                return self._create_individual_fallback_segments(subtitle_content, analysis_result)
            
            print(f"âœ… æå–åˆ° {len(individual_ranges)} ä¸ªç‹¬ç«‹çŸ¥è¯†ç‚¹æ—¶é—´èŒƒå›´")
            
            # 3. ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æ·»åŠ ç¼“å†²åŒºï¼ˆä½†ä¸åˆå¹¶ï¼‰
            print("ğŸ”§ ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æ·»åŠ ç‹¬ç«‹ç¼“å†²åŒº")
            max_duration = None
            if subtitle_lines and subtitle_lines[-1].get('timestamp'):
                max_duration = subtitle_lines[-1]['timestamp'] + 60  # æœ€åæ—¶é—´æˆ³ + 1åˆ†é’Ÿç¼“å†²
            
            buffered_ranges = self._add_individual_buffers(individual_ranges, max_duration)
            print(f"âœ… å¤„ç†åå¾—åˆ° {len(buffered_ranges)} ä¸ªç‹¬ç«‹æ—¶é—´æ®µ")
            
            # 4. ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æå–å¯¹åº”çš„æ–‡æœ¬å†…å®¹
            print("ğŸ“ ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æå–å¯¹åº”å­—å¹•æ–‡æœ¬")
            segments = self._extract_individual_knowledge_point_texts(subtitle_lines, buffered_ranges)
            
            # 5. ç»Ÿè®¡å’ŒéªŒè¯
            total_tokens = sum(seg.token_count for seg in segments)
            original_tokens = self._estimate_token_count(subtitle_content)
            
            print(f"ğŸ“Š ç‹¬ç«‹åˆ†æ®µç»Ÿè®¡:")
            print(f"  - çŸ¥è¯†ç‚¹æ•°é‡: {len(segments)}")
            print(f"  - åŸå§‹tokens: {original_tokens}")
            print(f"  - åˆ†æ®µåtokens: {total_tokens}")
            print(f"  - æ¯ä¸ªçŸ¥è¯†ç‚¹åŒ…å«ç¼“å†²åŒºå†…å®¹")
            
            return segments
            
        except Exception as e:
            print(f"âŒ æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µå¤±è´¥: {e}")
            return self._create_individual_fallback_segments(subtitle_content, analysis_result)
    
    def _extract_individual_knowledge_point_ranges(self, analysis_result: Dict[str, Any]) -> List[TimeRange]:
        """
        ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹åˆ›å»ºç‹¬ç«‹çš„æ—¶é—´èŒƒå›´
        
        Args:
            analysis_result: ç¬¬ä¸€æ­¥åˆ†æç»“æœ
            
        Returns:
            æ¯ä¸ªçŸ¥è¯†ç‚¹å¯¹åº”ä¸€ä¸ªç‹¬ç«‹æ—¶é—´èŒƒå›´çš„åˆ—è¡¨
        """
        individual_ranges = []
        
        # ä»knowledge_pointsä¸­æå–æ—¶é—´èŒƒå›´
        knowledge_points = analysis_result.get('knowledge_points', [])
        
        for kp in knowledge_points:
            kp_id = kp.get('id', 'unknown')
            time_range_str = kp.get('time_range', '')
            
            if time_range_str:
                time_range = TimeParser.parse_time_range(time_range_str, kp_id)
                if time_range:
                    # ç¡®ä¿æ¯ä¸ªæ—¶é—´èŒƒå›´åªåŒ…å«ä¸€ä¸ªçŸ¥è¯†ç‚¹
                    time_range.kp_ids = [kp_id]
                    individual_ranges.append(time_range)
                    print(f"  âœ… {kp_id}: {time_range.start:.1f}-{time_range.end:.1f}s")
                else:
                    print(f"  âš ï¸ æ— æ³•è§£ææ—¶é—´èŒƒå›´: {time_range_str} (çŸ¥è¯†ç‚¹: {kp_id})")
            else:
                print(f"  âš ï¸ çŸ¥è¯†ç‚¹ {kp_id} æ²¡æœ‰æ—¶é—´èŒƒå›´ä¿¡æ¯")
        
        return individual_ranges
    
    def _add_individual_buffers(self, time_ranges: List[TimeRange], max_duration: Optional[float] = None) -> List[TimeRange]:
        """
        ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹ç‹¬ç«‹æ·»åŠ ç¼“å†²åŒºï¼ˆä¸åˆå¹¶é‡å éƒ¨åˆ†ï¼‰
        
        Args:
            time_ranges: åŸå§‹æ—¶é—´èŒƒå›´åˆ—è¡¨
            max_duration: æœ€å¤§æ—¶é•¿é™åˆ¶
            
        Returns:
            æ·»åŠ ç¼“å†²åŒºåçš„æ—¶é—´èŒƒå›´åˆ—è¡¨
        """
        buffered_ranges = []
        
        for time_range in time_ranges:
            # æ·»åŠ ç¼“å†²åŒº
            buffered_start = max(0, time_range.start - self.buffer_seconds)
            buffered_end = time_range.end + self.buffer_seconds
            
            # åº”ç”¨æœ€å¤§æ—¶é•¿é™åˆ¶
            if max_duration is not None:
                buffered_end = min(buffered_end, max_duration)
            
            buffered_range = TimeRange(
                start=buffered_start,
                end=buffered_end,
                kp_ids=time_range.kp_ids.copy()
            )
            
            buffered_ranges.append(buffered_range)
            
            print(f"  ğŸ“ {time_range.kp_ids[0]}: {time_range.start:.1f}-{time_range.end:.1f}s â†’ {buffered_start:.1f}-{buffered_end:.1f}s (ç¼“å†²: Â±{self.buffer_seconds}s)")
        
        return buffered_ranges
    
    def _extract_individual_knowledge_point_texts(self, subtitle_lines: List[Dict], time_ranges: List[TimeRange]) -> List[Segment]:
        """
        ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æå–å¯¹åº”çš„å­—å¹•æ–‡æœ¬
        
        Args:
            subtitle_lines: è§£æåçš„å­—å¹•è¡Œ
            time_ranges: æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼ˆæ¯ä¸ªåŒ…å«ä¸€ä¸ªçŸ¥è¯†ç‚¹ï¼‰
            
        Returns:
            åˆ†æ®µç»“æœåˆ—è¡¨
        """
        segments = []
        
        for time_range in time_ranges:
            segment_text = []
            matched_lines = []
            kp_id = time_range.kp_ids[0]  # æ¯ä¸ªæ—¶é—´èŒƒå›´åªæœ‰ä¸€ä¸ªçŸ¥è¯†ç‚¹
            
            print(f"  ğŸ” å¤„ç†çŸ¥è¯†ç‚¹ {kp_id}: {time_range.start:.1f}-{time_range.end:.1f}s")
            
            # å¦‚æœæ˜¯çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆæ²¡æœ‰æ—¶é—´æˆ³ï¼‰ï¼Œä½¿ç”¨å®Œæ•´å†…å®¹
            if not subtitle_lines or not subtitle_lines[0].get('timestamp'):
                print(f"    ğŸ“ æ²¡æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œä½¿ç”¨å®Œæ•´å†…å®¹")
                full_text = '\n'.join([line.get('text', '') for line in subtitle_lines])
                segments.append(Segment(
                    text=full_text,
                    time_range=time_range,
                    knowledge_points=[kp_id],
                    token_count=self._estimate_token_count(full_text),
                    buffer_info={
                        'type': 'full_text',
                        'reason': 'no_timestamp',
                        'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s"
                    }
                ))
                continue
            
            # å¯¹äºæœ‰æ—¶é—´æˆ³çš„å­—å¹•ï¼ŒæŒ‰æ—¶é—´èŒƒå›´æå–
            for line in subtitle_lines:
                timestamp = line['timestamp']
                
                # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…
                if time_range.start <= timestamp <= time_range.end:
                    # ä¿ç•™åŸå§‹æ—¶é—´æˆ³ï¼ˆLRCæ ¼å¼ï¼‰
                    segment_text.append(f"[{line['original_time']}] {line['text']}")
                    matched_lines.append(line)
                # å¯¹äºSRTæ ¼å¼ï¼Œè¿˜è¦æ£€æŸ¥ç»“æŸæ—¶é—´
                elif 'end_timestamp' in line:
                    end_timestamp = line['end_timestamp']
                    # å¦‚æœå­—å¹•è¡Œä¸æ—¶é—´èŒƒå›´æœ‰é‡å ï¼Œåˆ™åŒ…å«
                    if not (end_timestamp < time_range.start or timestamp > time_range.end):
                        # ä¿ç•™åŸå§‹æ—¶é—´æˆ³ï¼ˆSRTæ ¼å¼ï¼‰
                        segment_text.append(f"[{line['original_time']}] {line['text']}")
                        matched_lines.append(line)
            
            # åˆ›å»ºåˆ†æ®µ
            if segment_text:
                text = '\n'.join(segment_text)
                buffer_info = {
                    'type': 'individual_segment',
                    'knowledge_point': kp_id,
                    'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s",
                    'matched_lines': len(matched_lines),
                    'buffer_added': self.buffer_seconds,
                    'first_timestamp': matched_lines[0]['timestamp'] if matched_lines else None,
                    'last_timestamp': matched_lines[-1]['timestamp'] if matched_lines else None
                }
                
                segments.append(Segment(
                    text=text,
                    time_range=time_range,
                    knowledge_points=[kp_id],
                    token_count=self._estimate_token_count(text),
                    buffer_info=buffer_info
                ))
                
                print(f"    âœ… æå–åˆ° {len(matched_lines)} è¡Œå­—å¹•ï¼Œ{self._estimate_token_count(text)} tokens")
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ–‡æœ¬ï¼Œåˆ›å»ºç©ºåˆ†æ®µå¹¶è®°å½•è­¦å‘Š
                print(f"    âš ï¸ æ²¡æœ‰åŒ¹é…çš„å­—å¹•å†…å®¹")
                segments.append(Segment(
                    text="",
                    time_range=time_range,
                    knowledge_points=[kp_id],
                    token_count=0,
                    buffer_info={
                        'type': 'empty_segment',
                        'knowledge_point': kp_id,
                        'reason': 'no_matching_text',
                        'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s"
                    }
                ))
        
        return segments
    
    def _create_individual_fallback_segments(self, subtitle_content: str, analysis_result: Dict[str, Any]) -> List[Segment]:
        """
        ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹åˆ›å»ºç‹¬ç«‹çš„fallbackåˆ†æ®µï¼ˆä½¿ç”¨å®Œæ•´å†…å®¹ï¼‰
        
        Args:
            subtitle_content: å®Œæ•´å­—å¹•å†…å®¹
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            æ¯ä¸ªçŸ¥è¯†ç‚¹å¯¹åº”ä¸€ä¸ªä½¿ç”¨å®Œæ•´å†…å®¹çš„åˆ†æ®µåˆ—è¡¨
        """
        knowledge_points = analysis_result.get('knowledge_points', [])
        segments = []
        
        print(f"ğŸ”„ åˆ›å»º {len(knowledge_points)} ä¸ªç‹¬ç«‹fallbackåˆ†æ®µ")
        
        for kp in knowledge_points:
            kp_id = kp.get('id', 'unknown')
            
            fallback_segment = Segment(
                text=subtitle_content,
                time_range=TimeRange(0, 0, [kp_id]),  # æ—¶é—´èŒƒå›´è®¾ä¸º0è¡¨ç¤ºå…¨éƒ¨å†…å®¹
                knowledge_points=[kp_id],
                token_count=self._estimate_token_count(subtitle_content),
                buffer_info={
                    'type': 'individual_fallback',
                    'knowledge_point': kp_id,
                    'reason': 'segmentation_failed',
                    'original_token_count': self._estimate_token_count(subtitle_content)
                }
            )
            
            segments.append(fallback_segment)
            print(f"  ğŸ“ çŸ¥è¯†ç‚¹ {kp_id}: ä½¿ç”¨å®Œæ•´å†…å®¹ ({self._estimate_token_count(subtitle_content)} tokens)")
        
        return segments
    
    def _estimate_token_count(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        Args:
            text: è¦ä¼°ç®—çš„æ–‡æœ¬
            
        Returns:
            ä¼°ç®—çš„tokenæ•°é‡
        """
        if not text:
            return 0
        
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æ•° + è‹±æ–‡å•è¯æ•° * 1.3
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        
        # è€ƒè™‘æ ‡ç‚¹ç¬¦å·å’Œå…¶ä»–å­—ç¬¦
        other_chars = len(text) - chinese_chars - sum(len(word) for word in re.findall(r'\b[a-zA-Z]+\b', text))
        
        estimated_tokens = chinese_chars + int(english_words * 1.3) + int(other_chars * 0.5)
        
        return max(estimated_tokens, 1)  # è‡³å°‘1ä¸ªtoken
    
    def segment_by_knowledge_points(self, subtitle_content: str, analysis_result: Dict[str, Any],
                              file_format: str = 'auto') -> List[Segment]:
        """
        æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µï¼ˆæ–°æ–¹æ³•ï¼‰
        æ¯ä¸ªçŸ¥è¯†ç‚¹å¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„æ®µè½ï¼Œæ·»åŠ ç¼“å†²åŒºä½†ä¸åˆå¹¶
        """
        try:
            print(f"ğŸ”§ æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µå¤„ç† (æ ¼å¼: {file_format})")
            
            # 1. è§£æå­—å¹•å†…å®¹
            subtitle_lines = self.subtitle_parser.parse_subtitle_content(subtitle_content, file_format)
            
            if not subtitle_lines:
                print("âš ï¸ å­—å¹•å†…å®¹è§£æä¸ºç©ºï¼Œå°†ä½¿ç”¨å®Œæ•´å†…å®¹ä½œä¸ºfallback")
                return self._create_individual_fallback_segments(subtitle_content, analysis_result)
            
            print(f"âœ… è§£æåˆ° {len(subtitle_lines)} è¡Œå­—å¹•")
            
            # 2. ä»åˆ†æç»“æœä¸­æå–çŸ¥è¯†ç‚¹æ—¶é—´èŒƒå›´
            print("ğŸ” æå–å„çŸ¥è¯†ç‚¹çš„æ—¶é—´èŒƒå›´")
            individual_ranges = self._extract_individual_knowledge_point_ranges(analysis_result)
            
            if not individual_ranges:
                print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´èŒƒå›´ï¼Œå°†ä½¿ç”¨å®Œæ•´å†…å®¹")
                return self._create_individual_fallback_segments(subtitle_content, analysis_result)
            
            print(f"âœ… æå–åˆ° {len(individual_ranges)} ä¸ªç‹¬ç«‹çŸ¥è¯†ç‚¹æ—¶é—´èŒƒå›´")
            
            # 3. ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æ·»åŠ ç‹¬ç«‹ç¼“å†²åŒºï¼ˆä½†ä¸åˆå¹¶ï¼‰
            print("ğŸ”§ ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æ·»åŠ ç‹¬ç«‹ç¼“å†²åŒº")
            max_duration = None
            if subtitle_lines and subtitle_lines[-1].get('timestamp'):
                max_duration = subtitle_lines[-1]['timestamp'] + 60
            
            buffered_ranges = self._add_individual_buffers(individual_ranges, max_duration)
            print(f"âœ… å¤„ç†åå¾—åˆ° {len(buffered_ranges)} ä¸ªç‹¬ç«‹æ—¶é—´æ®µ")
            
            # 4. ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æå–å¯¹åº”çš„æ–‡æœ¬å†…å®¹
            print("ğŸ“ ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æå–å¯¹åº”å­—å¹•æ–‡æœ¬")
            segments = self._extract_individual_knowledge_point_texts(subtitle_lines, buffered_ranges)
            
            # 5. ç»Ÿè®¡å’ŒéªŒè¯
            total_tokens = sum(seg.token_count for seg in segments)
            original_tokens = self._estimate_token_count(subtitle_content)
            
            print(f"ğŸ“Š ç‹¬ç«‹åˆ†æ®µç»Ÿè®¡:")
            print(f"  - çŸ¥è¯†ç‚¹æ•°é‡: {len(segments)}")
            print(f"  - åŸå§‹tokens: {original_tokens}")
            print(f"  - åˆ†æ®µåtokens: {total_tokens}")
            print(f"  - æ¯ä¸ªçŸ¥è¯†ç‚¹åŒ…å«ç¼“å†²åŒºå†…å®¹")
            
            return segments
            
        except Exception as e:
            print(f"âŒ æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µå¤±è´¥: {e}")
            return self._create_individual_fallback_segments(subtitle_content, analysis_result)
    
    # ä¿ç•™åŸæœ‰çš„æ–¹æ³•ä»¥å…¼å®¹ç°æœ‰ä»£ç 
    def segment_subtitle_content(self, subtitle_content: str, analysis_result: Dict[str, Any],
                                file_format: str = 'auto') -> List[Segment]:
        """
        æ™ºèƒ½åˆ†æ®µä¸»å…¥å£æ–¹æ³•ï¼ˆä¿æŒå…¼å®¹ï¼Œé»˜è®¤ä½¿ç”¨æ–°æ–¹æ³•ï¼‰
        
        Args:
            subtitle_content: å­—å¹•æ–‡ä»¶å†…å®¹
            analysis_result: ç¬¬ä¸€æ­¥åˆ†æç»“æœ
            file_format: å­—å¹•æ–‡ä»¶æ ¼å¼
            
        Returns:
            åˆ†æ®µç»“æœåˆ—è¡¨
        """
        # é»˜è®¤ä½¿ç”¨æ–°çš„æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µæ–¹æ³•
        return self.segment_by_knowledge_points(subtitle_content, analysis_result, file_format)
    
    def extract_time_ranges_from_analysis(self, analysis_result: Dict[str, Any]) -> List[TimeRange]:
        """
        ä»ç¬¬ä¸€æ­¥åˆ†æç»“æœä¸­æå–æ—¶é—´èŒƒå›´ï¼ˆä¿æŒå…¼å®¹ï¼‰
        
        Args:
            analysis_result: ç¬¬ä¸€æ­¥åˆ†æç»“æœ
            
        Returns:
            æ—¶é—´èŒƒå›´åˆ—è¡¨
        """
        return self._extract_individual_knowledge_point_ranges(analysis_result)
    
    def _extract_individual_knowledge_point_ranges(self, analysis_result: Dict[str, Any]) -> List[TimeRange]:
        """ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹åˆ›å»ºç‹¬ç«‹çš„æ—¶é—´èŒƒå›´"""
        individual_ranges = []
        knowledge_points = analysis_result.get('knowledge_points', [])
        
        for kp in knowledge_points:
            kp_id = kp.get('id', 'unknown')
            time_range_str = kp.get('time_range', '')
            
            if time_range_str:
                time_range = TimeParser.parse_time_range(time_range_str, kp_id)
                if time_range:
                    # ç¡®ä¿æ¯ä¸ªæ—¶é—´èŒƒå›´åªåŒ…å«ä¸€ä¸ªçŸ¥è¯†ç‚¹
                    time_range.kp_ids = [kp_id]
                    individual_ranges.append(time_range)
                    print(f"  âœ… {kp_id}: {time_range.start:.1f}-{time_range.end:.1f}s")
                else:
                    print(f"  âš ï¸ æ— æ³•è§£ææ—¶é—´èŒƒå›´: {time_range_str} (çŸ¥è¯†ç‚¹: {kp_id})")
            else:
                print(f"  âš ï¸ çŸ¥è¯†ç‚¹ {kp_id} æ²¡æœ‰æ—¶é—´èŒƒå›´ä¿¡æ¯")
        
        return individual_ranges

    def _add_individual_buffers(self, time_ranges: List[TimeRange], max_duration: Optional[float] = None) -> List[TimeRange]:
        """ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹ç‹¬ç«‹æ·»åŠ ç¼“å†²åŒºï¼ˆä¸åˆå¹¶é‡å éƒ¨åˆ†ï¼‰"""
        buffered_ranges = []
        
        for time_range in time_ranges:
            # æ·»åŠ ç¼“å†²åŒº
            buffered_start = max(0, time_range.start - self.buffer_seconds)
            buffered_end = time_range.end + self.buffer_seconds
            
            # åº”ç”¨æœ€å¤§æ—¶é•¿é™åˆ¶
            if max_duration is not None:
                buffered_end = min(buffered_end, max_duration)
            
            buffered_range = TimeRange(
                start=buffered_start,
                end=buffered_end,
                kp_ids=time_range.kp_ids.copy()
            )
            
            buffered_ranges.append(buffered_range)
            
            print(f"  ğŸ“ {time_range.kp_ids[0]}: {time_range.start:.1f}-{time_range.end:.1f}s â†’ {buffered_start:.1f}-{buffered_end:.1f}s (ç¼“å†²: Â±{self.buffer_seconds}s)")
        
        return buffered_ranges

    def _extract_individual_knowledge_point_texts(self, subtitle_lines: List[Dict], time_ranges: List[TimeRange]) -> List[Segment]:
        """ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹æå–å¯¹åº”çš„å­—å¹•æ–‡æœ¬"""
        segments = []
        
        for time_range in time_ranges:
            segment_text = []
            matched_lines = []
            kp_id = time_range.kp_ids[0]  # æ¯ä¸ªæ—¶é—´èŒƒå›´åªæœ‰ä¸€ä¸ªçŸ¥è¯†ç‚¹
            
            print(f"  ğŸ” å¤„ç†çŸ¥è¯†ç‚¹ {kp_id}: {time_range.start:.1f}-{time_range.end:.1f}s")
            
            # å¦‚æœæ˜¯çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆæ²¡æœ‰æ—¶é—´æˆ³ï¼‰ï¼Œä½¿ç”¨å®Œæ•´å†…å®¹
            if not subtitle_lines or not subtitle_lines[0].get('timestamp'):
                print(f"    ğŸ“ æ²¡æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œä½¿ç”¨å®Œæ•´å†…å®¹")
                full_text = '\n'.join([line.get('text', '') for line in subtitle_lines])
                segments.append(Segment(
                    text=full_text,
                    time_range=time_range,
                    knowledge_points=[kp_id],
                    token_count=self._estimate_token_count(full_text),
                    buffer_info={
                        'type': 'full_text',
                        'reason': 'no_timestamp',
                        'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s"
                    }
                ))
                continue
            
            # å¯¹äºæœ‰æ—¶é—´æˆ³çš„å­—å¹•ï¼ŒæŒ‰æ—¶é—´èŒƒå›´æå–
            for line in subtitle_lines:
                timestamp = line['timestamp']
                
                # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…
                if time_range.start <= timestamp <= time_range.end:
                    segment_text.append(line['text'])
                    matched_lines.append(line)
                # å¯¹äºSRTæ ¼å¼ï¼Œè¿˜è¦æ£€æŸ¥ç»“æŸæ—¶é—´
                elif 'end_timestamp' in line:
                    end_timestamp = line['end_timestamp']
                    # å¦‚æœå­—å¹•è¡Œä¸æ—¶é—´èŒƒå›´æœ‰é‡å ï¼Œåˆ™åŒ…å«
                    if not (end_timestamp < time_range.start or timestamp > time_range.end):
                        segment_text.append(line['text'])
                        matched_lines.append(line)
            
            # åˆ›å»ºåˆ†æ®µ
            if segment_text:
                text = '\n'.join(segment_text)
                buffer_info = {
                    'type': 'individual_segment',
                    'knowledge_point': kp_id,
                    'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s",
                    'matched_lines': len(matched_lines),
                    'buffer_added': self.buffer_seconds,
                    'first_timestamp': matched_lines[0]['timestamp'] if matched_lines else None,
                    'last_timestamp': matched_lines[-1]['timestamp'] if matched_lines else None
                }
                
                segments.append(Segment(
                    text=text,
                    time_range=time_range,
                    knowledge_points=[kp_id],
                    token_count=self._estimate_token_count(text),
                    buffer_info=buffer_info
                ))
                
                print(f"    âœ… æå–åˆ° {len(matched_lines)} è¡Œå­—å¹•ï¼Œ{self._estimate_token_count(text)} tokens")
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ–‡æœ¬ï¼Œåˆ›å»ºç©ºåˆ†æ®µå¹¶è®°å½•è­¦å‘Š
                print(f"    âš ï¸ æ²¡æœ‰åŒ¹é…çš„å­—å¹•å†…å®¹")
                segments.append(Segment(
                    text="",
                    time_range=time_range,
                    knowledge_points=[kp_id],
                    token_count=0,
                    buffer_info={
                        'type': 'empty_segment',
                        'knowledge_point': kp_id,
                        'reason': 'no_matching_text',
                        'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s"
                    }
                ))
        
        return segments

    def _create_individual_fallback_segments(self, subtitle_content: str, analysis_result: Dict[str, Any]) -> List[Segment]:
        """ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹åˆ›å»ºç‹¬ç«‹çš„fallbackåˆ†æ®µï¼ˆä½¿ç”¨å®Œæ•´å†…å®¹ï¼‰"""
        knowledge_points = analysis_result.get('knowledge_points', [])
        segments = []
        
        print(f"ğŸ”„ åˆ›å»º {len(knowledge_points)} ä¸ªç‹¬ç«‹fallbackåˆ†æ®µ")
        
        for kp in knowledge_points:
            kp_id = kp.get('id', 'unknown')
            
            fallback_segment = Segment(
                text=subtitle_content,
                time_range=TimeRange(0, 0, [kp_id]),  # æ—¶é—´èŒƒå›´è®¾ä¸º0è¡¨ç¤ºå…¨éƒ¨å†…å®¹
                knowledge_points=[kp_id],
                token_count=self._estimate_token_count(subtitle_content),
                buffer_info={
                    'type': 'individual_fallback',
                    'knowledge_point': kp_id,
                    'reason': 'segmentation_failed',
                    'original_token_count': self._estimate_token_count(subtitle_content)
                }
            )
            
            segments.append(fallback_segment)
            print(f"  ğŸ“ çŸ¥è¯†ç‚¹ {kp_id}: ä½¿ç”¨å®Œæ•´å†…å®¹ ({self._estimate_token_count(subtitle_content)} tokens)")
        
        return segments
    
    def get_segments_summary(self, segments: List[Segment]) -> Dict[str, Any]:
        """
        è·å–åˆ†æ®µç»“æœçš„æ‘˜è¦ä¿¡æ¯
        
        Args:
            segments: åˆ†æ®µç»“æœåˆ—è¡¨
            
        Returns:
            æ‘˜è¦ä¿¡æ¯å­—å…¸
        """
        if not segments:
            return {'total_segments': 0, 'total_tokens': 0, 'knowledge_points': []}
        
        total_tokens = sum(seg.token_count for seg in segments)
        all_kp_ids = []
        for seg in segments:
            all_kp_ids.extend(seg.knowledge_points)
        unique_kp_ids = list(set(all_kp_ids))
        
        time_ranges = []
        for seg in segments:
            if seg.time_range.start != seg.time_range.end:  # ä¸æ˜¯fallbackçš„æƒ…å†µ
                time_ranges.append({
                    'start': seg.time_range.start,
                    'end': seg.time_range.end,
                    'duration': seg.time_range.duration,
                    'knowledge_points': seg.knowledge_points
                })
        
        summary = {
            'total_segments': len(segments),
            'total_tokens': total_tokens,
            'knowledge_points': unique_kp_ids,
            'time_ranges': time_ranges,
            'total_duration': sum(tr['duration'] for tr in time_ranges),
            'avg_segment_tokens': total_tokens / len(segments) if segments else 0,
            'has_fallback': any(seg.buffer_info and seg.buffer_info.get('type', '').endswith('fallback') for seg in segments),
            'individual_segments': len([seg for seg in segments if seg.buffer_info and seg.buffer_info.get('type') == 'individual_segment']),
            'processing_mode': 'individual_knowledge_points'
        }
        
        return summary


def test_individual_segmentation():
    """æµ‹è¯•æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µåŠŸèƒ½")
    
    # æ¨¡æ‹Ÿç¬¬ä¸€æ­¥åˆ†æç»“æœ
    mock_analysis = {
        'knowledge_points': [
            {
                'id': 'KP001',
                'concept_name': 'ç‰©æƒçš„æ¦‚å¿µ',
                'time_range': '02:15.30-05:45.60'
            },
            {
                'id': 'KP002', 
                'concept_name': 'ç‰©æƒçš„ç‰¹å¾',
                'time_range': '06:00-08:30'
            },
            {
                'id': 'KP003',
                'concept_name': 'ç‰©æƒå˜åŠ¨',
                'time_range': '09:15-12:45'
            }
        ]
    }
    
    # æ¨¡æ‹ŸLRCæ ¼å¼å­—å¹•
    mock_lrc_content = """
[01:30.00]å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬æ¥è®²ç‰©æƒæ³•çš„åŸºç¡€æ¦‚å¿µ
[02:00.00]é¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ä»€ä¹ˆæ˜¯ç‰©æƒ
[02:15.30]ç‰©æƒæ˜¯æŒ‡æƒåˆ©äººä¾æ³•å¯¹ç‰¹å®šçš„ç‰©äº«æœ‰ç›´æ¥æ”¯é…å’Œæ’ä»–çš„æƒåˆ©
[03:00.00]è¿™ä¸ªå®šä¹‰åŒ…å«äº†å‡ ä¸ªå…³é”®è¦ç´ 
[03:30.00]ç¬¬ä¸€æ˜¯ç›´æ¥æ”¯é…æ€§ï¼Œç¬¬äºŒæ˜¯æ’ä»–æ€§
[04:00.00]æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹å…·ä½“çš„ä¾‹å­
[05:00.00]æ¯”å¦‚æ‰€æœ‰æƒå°±æ˜¯æœ€å…¸å‹çš„ç‰©æƒ
[06:00.00]ç‰©æƒæœ‰å“ªäº›ç‰¹å¾å‘¢
[06:30.00]ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯å¯¹ä¸–æ€§ï¼Œä¹Ÿå°±æ˜¯è¯´ç‰©æƒçš„æ•ˆåŠ›åŠäºä»»ä½•äºº
[07:00.00]ç¬¬äºŒä¸ªç‰¹å¾æ˜¯è¿½åŠæ€§ï¼Œç‰©æ— è®ºè½¬ç§»åˆ°å“ªé‡Œï¼Œç‰©æƒéƒ½èƒ½è¿½åŠ
[08:00.00]ç¬¬ä¸‰ä¸ªç‰¹å¾æ˜¯ä¼˜å…ˆæ€§ï¼Œç‰©æƒä¼˜å…ˆäºå€ºæƒ
[09:15.00]ç°åœ¨æˆ‘ä»¬æ¥è®²ç‰©æƒå˜åŠ¨
[10:00.00]ç‰©æƒå˜åŠ¨æ˜¯æŒ‡ç‰©æƒçš„è®¾ç«‹ã€å˜æ›´ã€è½¬è®©å’Œæ¶ˆç­
[11:00.00]æ ¹æ®ç‰©æƒæ³•çš„è§„å®šï¼Œä¸åŠ¨äº§ç‰©æƒå˜åŠ¨éœ€è¦ç™»è®°
[12:00.00]è€ŒåŠ¨äº§ç‰©æƒå˜åŠ¨ä¸€èˆ¬ä»¥äº¤ä»˜ä¸ºå‡†
"""
    
    # åˆ›å»ºåˆ†æ®µå™¨
    segmenter = IntelligentSegmenter(buffer_seconds=20.0)
    
    # æ‰§è¡ŒæŒ‰çŸ¥è¯†ç‚¹ç‹¬ç«‹åˆ†æ®µ
    segments = segmenter.segment_by_knowledge_points(
        mock_lrc_content, 
        mock_analysis, 
        file_format='lrc'
    )
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š ç‹¬ç«‹åˆ†æ®µç»“æœ:")
    for i, segment in enumerate(segments, 1):
        print(f"\nçŸ¥è¯†ç‚¹åˆ†æ®µ {i}:")
        print(f"  çŸ¥è¯†ç‚¹ID: {segment.knowledge_points[0]}")
        print(f"  æ—¶é—´èŒƒå›´: {segment.time_range.start:.1f}-{segment.time_range.end:.1f}s")
        print(f"  Tokenæ•°é‡: {segment.token_count}")
        print(f"  æ–‡æœ¬é•¿åº¦: {len(segment.text)} å­—ç¬¦")
        print(f"  æ–‡æœ¬é¢„è§ˆ: {segment.text[:100]}...")
        
        if segment.buffer_info:
            print(f"  å¤„ç†ç±»å‹: {segment.buffer_info.get('type', 'æœªçŸ¥')}")
    
    # è·å–æ‘˜è¦
    summary = segmenter.get_segments_summary(segments)
    print(f"\nğŸ“‹ ç‹¬ç«‹åˆ†æ®µæ‘˜è¦:")
    for key, value in summary.items():
        print(f"  {key}: {value}")
    
    print("\nâœ… ç‹¬ç«‹åˆ†æ®µæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_individual_segmentation()
