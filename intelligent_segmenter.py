"""
æ™ºèƒ½åˆ†æ®µæ ¸å¿ƒç±» - intelligent_segmenter.py

åŸºäºç¬¬ä¸€æ­¥åˆ†æç»“æœçš„time_rangeï¼Œç²¾å‡†æå–ç›¸å…³å­—å¹•ç‰‡æ®µï¼Œå‡å°‘60-80%çš„tokenä½¿ç”¨
"""

import re
import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from time_parser import TimeParser, TimeRange


@dataclass
class Segment:
    """å­—å¹•åˆ†æ®µæ•°æ®ç±»"""
    text: str                    # å­—å¹•æ–‡æœ¬å†…å®¹
    time_range: TimeRange        # æ—¶é—´èŒƒå›´
    knowledge_points: List[str]  # å…³è”çŸ¥è¯†ç‚¹ID
    token_count: int            # ä¼°ç®—tokenæ•°é‡
    buffer_info: Dict[str, Any] # ç¼“å†²åŒºä¿¡æ¯
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'text': self.text,
            'time_range': {
                'start': self.time_range.start,
                'end': self.time_range.end,
                'kp_ids': self.time_range.kp_ids
            },
            'knowledge_points': self.knowledge_points,
            'token_count': self.token_count,
            'buffer_info': self.buffer_info
        }


class SubtitleParser:
    """å­—å¹•æ–‡ä»¶è§£æå™¨"""
    
    @staticmethod
    def parse_lrc_content(content: str) -> List[Dict[str, Any]]:
        """
        è§£æLRCæ ¼å¼å­—å¹•
        
        Args:
            content: LRCå­—å¹•å†…å®¹
            
        Returns:
            æ—¶é—´æˆ³å’Œæ–‡æœ¬çš„åˆ—è¡¨
        """
        lines = []
        # LRCæ ¼å¼: [MM:SS.mm]æ–‡æœ¬å†…å®¹
        lrc_pattern = re.compile(r'\[(\d{1,2}:\d{2}(?:\.\d{1,3})?)\](.*)')
        
        for line in content.split('\n'):
            line = line.strip()
            if line:
                match = lrc_pattern.match(line)
                if match:
                    time_str, text = match.groups()
                    timestamp = TimeParser.parse_time_string(time_str)
                    if timestamp is not None and text.strip():
                        lines.append({
                            'timestamp': timestamp,
                            'text': text.strip(),
                            'original_time': time_str
                        })
        
        return sorted(lines, key=lambda x: x['timestamp'])
    
    @staticmethod
    def parse_srt_content(content: str) -> List[Dict[str, Any]]:
        """
        è§£æSRTæ ¼å¼å­—å¹•
        
        Args:
            content: SRTå­—å¹•å†…å®¹
            
        Returns:
            æ—¶é—´æˆ³å’Œæ–‡æœ¬çš„åˆ—è¡¨
        """
        lines = []
        # SRTæ ¼å¼åˆ†å—å¤„ç†
        blocks = re.split(r'\n\s*\n', content.strip())
        
        for block in blocks:
            block_lines = block.strip().split('\n')
            if len(block_lines) >= 3:
                # ç¬¬äºŒè¡Œæ˜¯æ—¶é—´æˆ³
                time_line = block_lines[1]
                # SRTæ—¶é—´æ ¼å¼: HH:MM:SS,mmm --> HH:MM:SS,mmm
                time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})', time_line)
                
                if time_match:
                    start_str, end_str = time_match.groups()
                    # è½¬æ¢é€—å·ä¸ºç‚¹å·
                    start_str = start_str.replace(',', '.')
                    end_str = end_str.replace(',', '.')
                    
                    start_time = TimeParser.parse_time_string(start_str)
                    end_time = TimeParser.parse_time_string(end_str)
                    
                    if start_time is not None and end_time is not None:
                        # åˆå¹¶æ–‡æœ¬è¡Œ
                        text = ' '.join(block_lines[2:]).strip()
                        if text:
                            lines.append({
                                'timestamp': start_time,
                                'end_timestamp': end_time,
                                'text': text,
                                'original_time': f"{start_str}-{end_str}"
                            })
        
        return sorted(lines, key=lambda x: x['timestamp'])
    
    @staticmethod
    def parse_txt_content(content: str) -> List[Dict[str, Any]]:
        """
        è§£æçº¯æ–‡æœ¬æ ¼å¼ï¼ˆæ— æ—¶é—´æˆ³ï¼‰
        
        Args:
            content: æ–‡æœ¬å†…å®¹
            
        Returns:
            æ–‡æœ¬è¡Œåˆ—è¡¨ï¼ˆæ—¶é—´æˆ³ä¸ºNoneï¼‰
        """
        lines = []
        for i, line in enumerate(content.split('\n')):
            line = line.strip()
            if line:
                lines.append({
                    'timestamp': None,
                    'text': line,
                    'line_number': i + 1
                })
        
        return lines
    
    @classmethod
    def parse_subtitle_content(cls, content: str, file_format: str = 'auto') -> List[Dict[str, Any]]:
        """
        è‡ªåŠ¨è¯†åˆ«å¹¶è§£æå­—å¹•å†…å®¹
        
        Args:
            content: å­—å¹•æ–‡ä»¶å†…å®¹
            file_format: æ–‡ä»¶æ ¼å¼ ('lrc', 'srt', 'txt', 'auto')
            
        Returns:
            è§£æåçš„å­—å¹•è¡Œåˆ—è¡¨
        """
        if not content or not content.strip():
            return []
        
        content = content.strip()
        
        # è‡ªåŠ¨è¯†åˆ«æ ¼å¼
        if file_format == 'auto':
            if re.search(r'\[\d{1,2}:\d{2}(?:\.\d{1,3})?\]', content):
                file_format = 'lrc'
            elif re.search(r'\d{2}:\d{2}:\d{2},\d{3}\s*-->\s*\d{2}:\d{2}:\d{2},\d{3}', content):
                file_format = 'srt'
            else:
                file_format = 'txt'
        
        # æ ¹æ®æ ¼å¼è§£æ
        if file_format == 'lrc':
            return cls.parse_lrc_content(content)
        elif file_format == 'srt':
            return cls.parse_srt_content(content)
        else:  # txt æˆ–å…¶ä»–
            return cls.parse_txt_content(content)


class IntelligentSegmenter:
    """æ™ºèƒ½åˆ†æ®µå¤„ç†å™¨"""
    
    def __init__(self, buffer_seconds: float = 30.0, max_gap_seconds: float = 5.0):
        """
        åˆå§‹åŒ–åˆ†æ®µå™¨
        
        Args:
            buffer_seconds: ç¼“å†²åŒºå¤§å°ï¼ˆç§’ï¼‰
            max_gap_seconds: æœ€å¤§é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå°äºæ­¤å€¼çš„æ—¶é—´æ®µå°†åˆå¹¶
        """
        self.buffer_seconds = buffer_seconds
        self.max_gap_seconds = max_gap_seconds
        self.subtitle_parser = SubtitleParser()
    
    def extract_time_ranges_from_analysis(self, analysis_result: Dict[str, Any]) -> List[TimeRange]:
        """
        ä»ç¬¬ä¸€æ­¥åˆ†æç»“æœä¸­æå–æ—¶é—´èŒƒå›´
        
        Args:
            analysis_result: ç¬¬ä¸€æ­¥åˆ†æç»“æœ
            
        Returns:
            æ—¶é—´èŒƒå›´åˆ—è¡¨
        """
        time_ranges = []
        
        # ä»knowledge_pointsä¸­æå–æ—¶é—´èŒƒå›´
        knowledge_points = analysis_result.get('knowledge_points', [])
        
        for kp in knowledge_points:
            kp_id = kp.get('id', 'unknown')
            time_range_str = kp.get('time_range', '')
            
            if time_range_str:
                time_range = TimeParser.parse_time_range(time_range_str, kp_id)
                if time_range:
                    time_ranges.append(time_range)
                else:
                    print(f"âš ï¸ æ— æ³•è§£ææ—¶é—´èŒƒå›´: {time_range_str} (çŸ¥è¯†ç‚¹: {kp_id})")
        
        return time_ranges
    
    def add_buffer_and_merge_ranges(self, time_ranges: List[TimeRange], 
                                   max_duration: float = None) -> List[TimeRange]:
        """
        ä¸ºæ—¶é—´èŒƒå›´æ·»åŠ ç¼“å†²åŒºå¹¶åˆå¹¶é‡å /ç›¸é‚»çš„èŒƒå›´
        
        Args:
            time_ranges: åŸå§‹æ—¶é—´èŒƒå›´åˆ—è¡¨
            max_duration: å­—å¹•æœ€å¤§æ—¶é•¿ï¼ˆç”¨äºè¾¹ç•Œæ§åˆ¶ï¼‰
            
        Returns:
            å¤„ç†åçš„æ—¶é—´èŒƒå›´åˆ—è¡¨
        """
        if not time_ranges:
            return []
        
        # æ·»åŠ ç¼“å†²åŒº
        buffered_ranges = TimeParser.add_buffer_to_ranges(
            time_ranges, 
            buffer_seconds=self.buffer_seconds,
            min_time=0.0,
            max_time=max_duration
        )
        
        # æ ‡å‡†åŒ–ï¼ˆåˆå¹¶é‡å å’Œç›¸é‚»çš„èŒƒå›´ï¼‰
        normalized_ranges = TimeParser.normalize_time_ranges(buffered_ranges)
        
        return normalized_ranges
    
    def extract_text_by_time_ranges(self, subtitle_lines: List[Dict[str, Any]], 
                                   time_ranges: List[TimeRange]) -> List[Segment]:
        """
        æ ¹æ®æ—¶é—´èŒƒå›´æå–å¯¹åº”çš„å­—å¹•æ–‡æœ¬
        
        Args:
            subtitle_lines: è§£æåçš„å­—å¹•è¡Œåˆ—è¡¨
            time_ranges: ç›®æ ‡æ—¶é—´èŒƒå›´åˆ—è¡¨
            
        Returns:
            åˆ†æ®µç»“æœåˆ—è¡¨
        """
        segments = []
        
        for time_range in time_ranges:
            segment_text = []
            matched_lines = []
            
            # å¦‚æœå­—å¹•æ²¡æœ‰æ—¶é—´æˆ³ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰
            if not subtitle_lines or subtitle_lines[0].get('timestamp') is None:
                # å¯¹äºçº¯æ–‡æœ¬ï¼Œè¿”å›å®Œæ•´å†…å®¹ä½œä¸ºä¸€ä¸ªåˆ†æ®µ
                full_text = '\n'.join([line['text'] for line in subtitle_lines])
                segments.append(Segment(
                    text=full_text,
                    time_range=time_range,
                    knowledge_points=time_range.kp_ids.copy(),
                    token_count=self._estimate_token_count(full_text),
                    buffer_info={
                        'type': 'full_text',
                        'reason': 'no_timestamp',
                        'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s"
                    }
                ))
                continue
            
            # å¯¹äºæœ‰æ—¶é—´æˆ³çš„å­—å¹•ï¼ŒæŒ‰æ—¶é—´èŒƒå›´æå–
            for line in subtitle_lines:
                timestamp = line['timestamp']
                
                # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…
                if time_range.start <= timestamp <= time_range.end:
                    segment_text.append(line['text'])
                    matched_lines.append(line)
                # å¯¹äºSRTæ ¼å¼ï¼Œè¿˜è¦æ£€æŸ¥ç»“æŸæ—¶é—´
                elif 'end_timestamp' in line:
                    end_timestamp = line['end_timestamp']
                    # å¦‚æœå­—å¹•è¡Œä¸æ—¶é—´èŒƒå›´æœ‰é‡å ï¼Œåˆ™åŒ…å«
                    if not (end_timestamp < time_range.start or timestamp > time_range.end):
                        segment_text.append(line['text'])
                        matched_lines.append(line)
            
            # åˆ›å»ºåˆ†æ®µ
            if segment_text:
                text = '\n'.join(segment_text)
                buffer_info = {
                    'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s",
                    'matched_lines': len(matched_lines),
                    'buffer_added': self.buffer_seconds,
                    'first_timestamp': matched_lines[0]['timestamp'] if matched_lines else None,
                    'last_timestamp': matched_lines[-1]['timestamp'] if matched_lines else None
                }
                
                segments.append(Segment(
                    text=text,
                    time_range=time_range,
                    knowledge_points=time_range.kp_ids.copy(),
                    token_count=self._estimate_token_count(text),
                    buffer_info=buffer_info
                ))
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ–‡æœ¬ï¼Œåˆ›å»ºç©ºåˆ†æ®µå¹¶è®°å½•è­¦å‘Š
                print(f"âš ï¸ æ—¶é—´èŒƒå›´ {time_range.start:.1f}-{time_range.end:.1f}s æ²¡æœ‰åŒ¹é…çš„å­—å¹•å†…å®¹")
                segments.append(Segment(
                    text="",
                    time_range=time_range,
                    knowledge_points=time_range.kp_ids.copy(),
                    token_count=0,
                    buffer_info={
                        'type': 'empty_segment',
                        'reason': 'no_matching_text',
                        'original_range': f"{time_range.start:.1f}-{time_range.end:.1f}s"
                    }
                ))
        
        return segments
    
    def _estimate_token_count(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        Args:
            text: è¦ä¼°ç®—çš„æ–‡æœ¬
            
        Returns:
            ä¼°ç®—çš„tokenæ•°é‡
        """
        if not text:
            return 0
        
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æ•° + è‹±æ–‡å•è¯æ•° * 1.3
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        
        # è€ƒè™‘æ ‡ç‚¹ç¬¦å·å’Œå…¶ä»–å­—ç¬¦
        other_chars = len(text) - chinese_chars - sum(len(word) for word in re.findall(r'\b[a-zA-Z]+\b', text))
        
        estimated_tokens = chinese_chars + int(english_words * 1.3) + int(other_chars * 0.5)
        
        return max(estimated_tokens, 1)  # è‡³å°‘1ä¸ªtoken
    
    def segment_subtitle_content(self, subtitle_content: str, analysis_result: Dict[str, Any],
                                file_format: str = 'auto') -> List[Segment]:
        """
        æ™ºèƒ½åˆ†æ®µä¸»å…¥å£æ–¹æ³•
        
        Args:
            subtitle_content: å­—å¹•æ–‡ä»¶å†…å®¹
            analysis_result: ç¬¬ä¸€æ­¥åˆ†æç»“æœ
            file_format: å­—å¹•æ–‡ä»¶æ ¼å¼
            
        Returns:
            åˆ†æ®µç»“æœåˆ—è¡¨
        """
        try:
            # 1. è§£æå­—å¹•å†…å®¹
            print(f"ğŸ“– è§£æå­—å¹•å†…å®¹ (æ ¼å¼: {file_format})")
            subtitle_lines = self.subtitle_parser.parse_subtitle_content(subtitle_content, file_format)
            
            if not subtitle_lines:
                print("âš ï¸ å­—å¹•å†…å®¹è§£æä¸ºç©ºï¼Œå°†ä½¿ç”¨å®Œæ•´å†…å®¹")
                return self._create_fallback_segment(subtitle_content, analysis_result)
            
            print(f"âœ… è§£æåˆ° {len(subtitle_lines)} è¡Œå­—å¹•")
            
            # 2. ä»åˆ†æç»“æœä¸­æå–æ—¶é—´èŒƒå›´
            print("ğŸ” æå–çŸ¥è¯†ç‚¹æ—¶é—´èŒƒå›´")
            time_ranges = self.extract_time_ranges_from_analysis(analysis_result)
            
            if not time_ranges:
                print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´èŒƒå›´ï¼Œå°†ä½¿ç”¨å®Œæ•´å†…å®¹")
                return self._create_fallback_segment(subtitle_content, analysis_result)
            
            print(f"âœ… æå–åˆ° {len(time_ranges)} ä¸ªæ—¶é—´èŒƒå›´")
            
            # 3. æ·»åŠ ç¼“å†²åŒºå¹¶åˆå¹¶èŒƒå›´
            print("ğŸ”§ æ·»åŠ ç¼“å†²åŒºå¹¶åˆå¹¶é‡å èŒƒå›´")
            max_duration = None
            if subtitle_lines and subtitle_lines[-1].get('timestamp'):
                max_duration = subtitle_lines[-1]['timestamp'] + 60  # æœ€åæ—¶é—´æˆ³ + 1åˆ†é’Ÿç¼“å†²
            
            processed_ranges = self.add_buffer_and_merge_ranges(time_ranges, max_duration)
            print(f"âœ… å¤„ç†åå¾—åˆ° {len(processed_ranges)} ä¸ªæ—¶é—´æ®µ")
            
            # 4. æå–å¯¹åº”çš„æ–‡æœ¬å†…å®¹
            print("ğŸ“ æå–å¯¹åº”æ—¶é—´æ®µçš„å­—å¹•æ–‡æœ¬")
            segments = self.extract_text_by_time_ranges(subtitle_lines, processed_ranges)
            
            # 5. ç»Ÿè®¡å’ŒéªŒè¯
            total_tokens = sum(seg.token_count for seg in segments)
            original_tokens = self._estimate_token_count(subtitle_content)
            reduction_ratio = (1 - total_tokens / original_tokens) * 100 if original_tokens > 0 else 0
            
            print(f"ğŸ“Š åˆ†æ®µç»Ÿè®¡:")
            print(f"  - åˆ†æ®µæ•°é‡: {len(segments)}")
            print(f"  - åŸå§‹tokens: {original_tokens}")
            print(f"  - åˆ†æ®µåtokens: {total_tokens}")
            print(f"  - Tokenå‡å°‘: {reduction_ratio:.1f}%")
            
            # éªŒè¯åˆ†æ®µæ•ˆæœ
            if reduction_ratio < 10:  # å¦‚æœå‡å°‘æ¯”ä¾‹å°äº10%ï¼Œç»™å‡ºè­¦å‘Š
                print("âš ï¸ Tokenå‡å°‘æ¯”ä¾‹è¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç¼“å†²åŒºå‚æ•°")
            
            return segments
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½åˆ†æ®µå¤±è´¥: {e}")
            return self._create_fallback_segment(subtitle_content, analysis_result)
    
    def _create_fallback_segment(self, subtitle_content: str, analysis_result: Dict[str, Any]) -> List[Segment]:
        """
        åˆ›å»ºfallbackåˆ†æ®µï¼ˆä½¿ç”¨å®Œæ•´å†…å®¹ï¼‰
        
        Args:
            subtitle_content: å®Œæ•´å­—å¹•å†…å®¹
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„åˆ†æ®µåˆ—è¡¨
        """
        knowledge_points = analysis_result.get('knowledge_points', [])
        kp_ids = [kp.get('id', 'unknown') for kp in knowledge_points]
        
        fallback_segment = Segment(
            text=subtitle_content,
            time_range=TimeRange(0, 0, kp_ids),  # æ—¶é—´èŒƒå›´è®¾ä¸º0è¡¨ç¤ºå…¨éƒ¨å†…å®¹
            knowledge_points=kp_ids,
            token_count=self._estimate_token_count(subtitle_content),
            buffer_info={
                'type': 'fallback',
                'reason': 'segmentation_failed',
                'original_token_count': self._estimate_token_count(subtitle_content)
            }
        )
        
        return [fallback_segment]
    
    def get_segments_summary(self, segments: List[Segment]) -> Dict[str, Any]:
        """
        è·å–åˆ†æ®µç»“æœçš„æ‘˜è¦ä¿¡æ¯
        
        Args:
            segments: åˆ†æ®µç»“æœåˆ—è¡¨
            
        Returns:
            æ‘˜è¦ä¿¡æ¯å­—å…¸
        """
        if not segments:
            return {'total_segments': 0, 'total_tokens': 0, 'knowledge_points': []}
        
        total_tokens = sum(seg.token_count for seg in segments)
        all_kp_ids = []
        for seg in segments:
            all_kp_ids.extend(seg.knowledge_points)
        unique_kp_ids = list(set(all_kp_ids))
        
        time_ranges = []
        for seg in segments:
            if seg.time_range.start != seg.time_range.end:  # ä¸æ˜¯fallbackçš„æƒ…å†µ
                time_ranges.append({
                    'start': seg.time_range.start,
                    'end': seg.time_range.end,
                    'duration': seg.time_range.duration
                })
        
        summary = {
            'total_segments': len(segments),
            'total_tokens': total_tokens,
            'knowledge_points': unique_kp_ids,
            'time_ranges': time_ranges,
            'total_duration': sum(tr['duration'] for tr in time_ranges),
            'avg_segment_tokens': total_tokens / len(segments) if segments else 0,
            'has_fallback': any(seg.buffer_info.get('type') == 'fallback' for seg in segments)
        }
        
        return summary


def test_intelligent_segmenter():
    """æµ‹è¯•æ™ºèƒ½åˆ†æ®µå™¨åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½åˆ†æ®µå™¨")
    
    # æ¨¡æ‹Ÿç¬¬ä¸€æ­¥åˆ†æç»“æœ
    mock_analysis = {
        'knowledge_points': [
            {
                'id': 'KP001',
                'concept_name': 'ç‰©æƒçš„æ¦‚å¿µ',
                'time_range': '02:15.30-05:45.60'
            },
            {
                'id': 'KP002', 
                'concept_name': 'ç‰©æƒçš„ç‰¹å¾',
                'time_range': '06:00-08:30'
            },
            {
                'id': 'KP003',
                'concept_name': 'ç‰©æƒå˜åŠ¨',
                'time_range': '09:15-12:45'
            }
        ]
    }
    
    # æ¨¡æ‹ŸLRCæ ¼å¼å­—å¹•
    mock_lrc_content = """
[01:30.00]å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬æ¥è®²ç‰©æƒæ³•çš„åŸºç¡€æ¦‚å¿µ
[02:00.00]é¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ä»€ä¹ˆæ˜¯ç‰©æƒ
[02:15.30]ç‰©æƒæ˜¯æŒ‡æƒåˆ©äººä¾æ³•å¯¹ç‰¹å®šçš„ç‰©äº«æœ‰ç›´æ¥æ”¯é…å’Œæ’ä»–çš„æƒåˆ©
[03:00.00]è¿™ä¸ªå®šä¹‰åŒ…å«äº†å‡ ä¸ªå…³é”®è¦ç´ 
[03:30.00]ç¬¬ä¸€æ˜¯ç›´æ¥æ”¯é…æ€§ï¼Œç¬¬äºŒæ˜¯æ’ä»–æ€§
[04:00.00]æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹å…·ä½“çš„ä¾‹å­
[05:00.00]æ¯”å¦‚æ‰€æœ‰æƒå°±æ˜¯æœ€å…¸å‹çš„ç‰©æƒ
[06:00.00]ç‰©æƒæœ‰å“ªäº›ç‰¹å¾å‘¢
[06:30.00]ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯å¯¹ä¸–æ€§ï¼Œä¹Ÿå°±æ˜¯è¯´ç‰©æƒçš„æ•ˆåŠ›åŠäºä»»ä½•äºº
[07:00.00]ç¬¬äºŒä¸ªç‰¹å¾æ˜¯è¿½åŠæ€§ï¼Œç‰©æ— è®ºè½¬ç§»åˆ°å“ªé‡Œï¼Œç‰©æƒéƒ½èƒ½è¿½åŠ
[08:00.00]ç¬¬ä¸‰ä¸ªç‰¹å¾æ˜¯ä¼˜å…ˆæ€§ï¼Œç‰©æƒä¼˜å…ˆäºå€ºæƒ
[09:15.00]ç°åœ¨æˆ‘ä»¬æ¥è®²ç‰©æƒå˜åŠ¨
[10:00.00]ç‰©æƒå˜åŠ¨æ˜¯æŒ‡ç‰©æƒçš„è®¾ç«‹ã€å˜æ›´ã€è½¬è®©å’Œæ¶ˆç­
[11:00.00]æ ¹æ®ç‰©æƒæ³•çš„è§„å®šï¼Œä¸åŠ¨äº§ç‰©æƒå˜åŠ¨éœ€è¦ç™»è®°
[12:00.00]è€ŒåŠ¨äº§ç‰©æƒå˜åŠ¨ä¸€èˆ¬ä»¥äº¤ä»˜ä¸ºå‡†
"""
    
    # åˆ›å»ºåˆ†æ®µå™¨
    segmenter = IntelligentSegmenter(buffer_seconds=20.0)
    
    # æ‰§è¡Œåˆ†æ®µ
    segments = segmenter.segment_subtitle_content(
        mock_lrc_content, 
        mock_analysis, 
        file_format='lrc'
    )
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š åˆ†æ®µç»“æœ:")
    for i, segment in enumerate(segments, 1):
        print(f"\nåˆ†æ®µ {i}:")
        print(f"  æ—¶é—´èŒƒå›´: {segment.time_range.start:.1f}-{segment.time_range.end:.1f}s")
        print(f"  å…³è”çŸ¥è¯†ç‚¹: {segment.knowledge_points}")
        print(f"  Tokenæ•°é‡: {segment.token_count}")
        print(f"  æ–‡æœ¬é•¿åº¦: {len(segment.text)} å­—ç¬¦")
        print(f"  æ–‡æœ¬é¢„è§ˆ: {segment.text[:100]}...")
    
    # è·å–æ‘˜è¦
    summary = segmenter.get_segments_summary(segments)
    print(f"\nğŸ“‹ åˆ†æ®µæ‘˜è¦:")
    for key, value in summary.items():
        print(f"  {key}: {value}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_intelligent_segmenter()
