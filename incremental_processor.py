import os
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional

class IncrementalProcessor:
    """å¢é‡å¤„ç†ç®¡ç†å™¨ - æ™ºèƒ½æ£€æµ‹å“ªäº›ç¬”è®°éœ€è¦é‡æ–°å¢å¼º"""
    
    def __init__(self, vault_path: str):
        self.vault_path = vault_path
        self.tracking_file = os.path.join(vault_path, "note_change_tracking.json")
        self.tracking_data = {
            'last_full_enhancement': None,
            'note_hashes': {},       # æ–‡ä»¶è·¯å¾„ -> å†…å®¹å“ˆå¸Œ
            'concept_count_at_last_enhancement': 0,
            'enhancement_history': []
        }
        self.load_tracking_data()
    
    def load_tracking_data(self) -> None:
        """åŠ è½½è¿½è¸ªæ•°æ®"""
        try:
            if os.path.exists(self.tracking_file):
                with open(self.tracking_file, 'r', encoding='utf-8') as f:
                    self.tracking_data = json.load(f)
                print(f"ğŸ“– åŠ è½½å¢é‡è¿½è¸ªæ•°æ®: {len(self.tracking_data['note_hashes'])} ä¸ªç¬”è®°è®°å½•")
            else:
                print("âœ¨ åˆå§‹åŒ–å¢é‡è¿½è¸ªç³»ç»Ÿ")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è¿½è¸ªæ•°æ®å¤±è´¥: {e}")
    
    def save_tracking_data(self) -> None:
        """ä¿å­˜è¿½è¸ªæ•°æ®"""
        try:
            with open(self.tracking_file, 'w', encoding='utf-8') as f:
                json.dump(self.tracking_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜è¿½è¸ªæ•°æ®å¤±è´¥: {e}")
    
    def get_notes_needing_enhancement(self, all_notes: List[Dict], concept_manager) -> List[Dict]:
        """è¯†åˆ«éœ€è¦å¢å¼ºçš„ç¬”è®°"""
        current_concept_count = len(concept_manager.concept_database) if concept_manager.concept_database else 0
        notes_to_process = []
        
        # æƒ…å†µ1ï¼šæ¦‚å¿µåº“æœ‰æ–°å¢æ¦‚å¿µ
        if current_concept_count > self.tracking_data['concept_count_at_last_enhancement']:
            new_concept_count = current_concept_count - self.tracking_data['concept_count_at_last_enhancement']
            print(f"ğŸ†• æ£€æµ‹åˆ° {new_concept_count} ä¸ªæ–°æ¦‚å¿µï¼Œå¯»æ‰¾ç›¸å…³ç¬”è®°...")
            
            # ç®€åŒ–ç­–ç•¥ï¼šå¦‚æœæ–°æ¦‚å¿µè¾ƒå¤šï¼Œå¤„ç†æ‰€æœ‰ç¬”è®°ï¼›å¦‚æœè¾ƒå°‘ï¼Œåªå¤„ç†å˜åŒ–çš„ç¬”è®°
            if new_concept_count > 10:
                print("ğŸ“ æ–°æ¦‚å¿µè¾ƒå¤šï¼Œå»ºè®®å…¨é‡å¤„ç†")
                return all_notes
        
        # æƒ…å†µ2ï¼šç¬”è®°å†…å®¹å‘ç”Ÿå˜åŒ–
        changed_notes = []
        for note in all_notes:
            if self._note_content_changed(note):
                changed_notes.append(note)
                notes_to_process.append(note)
        
        if changed_notes:
            print(f"ğŸ“ æ£€æµ‹åˆ° {len(changed_notes)} ä¸ªç¬”è®°å†…å®¹å˜åŒ–")
        
        # å¦‚æœæ²¡æœ‰å˜åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not notes_to_process:
            print("âœ… æ‰€æœ‰ç¬”è®°éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€å¢å¼º")
        
        return notes_to_process
    
    def _note_content_changed(self, note: Dict) -> bool:
        """æ£€æŸ¥ç¬”è®°å†…å®¹æ˜¯å¦å‘ç”Ÿå˜åŒ–"""
        file_path = note['file_path']
        current_hash = self._get_content_hash(note['content'])
        stored_hash = self.tracking_data['note_hashes'].get(file_path)
        
        return stored_hash != current_hash
    
    def _get_content_hash(self, content: str) -> str:
        """è·å–å†…å®¹çš„MD5å“ˆå¸Œå€¼"""
        # æ ‡å‡†åŒ–å†…å®¹åè®¡ç®—å“ˆå¸Œ
        normalized_content = self._normalize_content_for_hash(content)
        return hashlib.md5(normalized_content.encode('utf-8')).hexdigest()
    
    def _normalize_content_for_hash(self, content: str) -> str:
        """æ ‡å‡†åŒ–å†…å®¹ç”¨äºå“ˆå¸Œè®¡ç®—"""
        import re
        
        # ç§»é™¤æ—¶é—´æˆ³ã€æ—¥æœŸç­‰å®¹æ˜“å˜åŒ–çš„å†…å®¹
        content = re.sub(r'\d{4}-\d{2}-\d{2}', '', content)
        content = re.sub(r'\d{2}:\d{2}:\d{2}', '', content)
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content.strip())
        
        return content
    
    def update_tracking_after_enhancement(self, processed_notes: List[Dict], concept_manager) -> None:
        """å¢å¼ºå®Œæˆåæ›´æ–°è¿½è¸ªä¿¡æ¯"""
        # æ›´æ–°ç¬”è®°å“ˆå¸Œ
        for note in processed_notes:
            file_path = note['file_path']
            content_hash = self._get_content_hash(note['content'])
            self.tracking_data['note_hashes'][file_path] = content_hash
        
        # æ›´æ–°æ¦‚å¿µæ•°é‡
        current_concept_count = len(concept_manager.concept_database) if concept_manager.concept_database else 0
        self.tracking_data['concept_count_at_last_enhancement'] = current_concept_count
        self.tracking_data['last_full_enhancement'] = datetime.now().isoformat()
        
        # è®°å½•å¢å¼ºå†å²
        self.tracking_data['enhancement_history'].append({
            'timestamp': datetime.now().isoformat(),
            'processed_notes': len(processed_notes),
            'total_concepts': current_concept_count,
            'type': 'incremental' if processed_notes else 'full'
        })
        
        self.save_tracking_data()
        print(f"ğŸ“Š å¢é‡è¿½è¸ªå·²æ›´æ–°: {len(processed_notes)} ä¸ªç¬”è®°, {current_concept_count} ä¸ªæ¦‚å¿µ")
    
    def force_full_rebuild(self) -> None:
        """å¼ºåˆ¶é‡ç½®è¿½è¸ªçŠ¶æ€ï¼Œä¸‹æ¬¡å°†è¿›è¡Œå®Œæ•´å¢å¼º"""
        self.tracking_data['note_hashes'] = {}
        self.tracking_data['concept_count_at_last_enhancement'] = 0
        self.tracking_data['last_full_enhancement'] = None
        self.save_tracking_data()
        print("ğŸ”„ å·²é‡ç½®å¢é‡è¿½è¸ªçŠ¶æ€ï¼Œä¸‹æ¬¡å°†è¿›è¡Œå®Œæ•´å¢å¼º")